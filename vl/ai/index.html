<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xml:lang="en" lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Style-Type" content="text/css" />
    <title>target</title>
    <link rel="stylesheet" type="text/css" href="./target/target.css" />
    <!--[if IE]><script type="text/javascript" src="./target/excanvas-compiled.js"></script><![endif]-->
    <script type="text/javascript" src="./target/target.js"> </script>
  </head>
  <body>
    <div style="margin:1ex;">
      <div style="width:100%">
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 1</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-6"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:37.908pt;z-index:1;letter-spacing:-.003em;">IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART C: APPLICATIONS AND REVIEWS, VOL. 38, NO. 2, MARCH 2008</div><div style="position:absolute;top:34.3462pt;left:541.7pt;z-index:1;">145</div></span><span class="fmt-1" style="white-space:pre;"><div style="position:absolute;top:59.1079pt;left:102.35pt;z-index:2;">Gait Components and Their Application</div></span><span class="fmt-1" style="white-space:pre;"><div style="position:absolute;top:87.0089pt;left:186.03pt;z-index:3;">to Gender Recognition</div></span><span class="fmt-2" style="white-space:pre;"><div style="position:absolute;top:121.46pt;left:269.98pt;z-index:6;">, Stephen J. Maybank</div></span><span class="fmt-3" style="white-space:pre;"><div style="position:absolute;top:134.41pt;left:292.44pt;z-index:12;letter-spacing:-.008em;">, Member, IEEE</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:757pt;left:242.99pt;z-index:115;">1094-6977/$25.00 © 2008 IEEE</div></span><span class="fmt-2" style="white-space:pre;"><div style="position:absolute;top:121.46pt;left:118.41pt;z-index:5;letter-spacing:-.003em;">Xuelong Li<span class="fmt-3">, Senior Member, IEEE</span></div></span><span class="fmt-2" style="white-space:pre;"><div style="position:absolute;top:134.41pt;left:92.9424pt;z-index:11;letter-spacing:-.007em;">Shuicheng Yan<span class="fmt-3">, Member, IEEE</span>, Dacheng Tao</div></span><span class="fmt-4" style="white-space:pre;"><div style="position:absolute;top:178.62pt;left:47.871pt;z-index:16;letter-spacing:-.011em;">Abstract<span class="fmt-5">—Human  gait  is  a  promising  biometrics  resource.  In</span></div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:188.59pt;left:37.9084pt;z-index:17;letter-spacing:-.008em;">this paper, the information about gait is obtained from the motions</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:198.55pt;left:37.9084pt;z-index:18;letter-spacing:.001em;">of  the  different  parts  of  the  silhouette.  The  human  silhouette</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:208.51pt;left:37.9084pt;z-index:19;letter-spacing:-.01em;">is  segmented  into  seven  components,  namely  head,  arm,  trunk,</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:218.48pt;left:37.9084pt;z-index:20;letter-spacing:-.003em;">thigh,  front-leg,  back-leg,  and  feet.  The  leg  silhouettes  for  the</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:228.44pt;left:37.9084pt;z-index:21;letter-spacing:.004em;">front-leg  and  the  back-leg  are  considered  separately  because,</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:238.4pt;left:37.9084pt;z-index:22;letter-spacing:-.018em;">during  walking,  the  left  leg  and  the  right  leg  are  in  front  or  at</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:248.37pt;left:37.9084pt;z-index:23;letter-spacing:.017em;">the back by turns. Each of the seven components and a number</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:258.33pt;left:37.9084pt;z-index:24;letter-spacing:.012em;">of combinations of the components are then studied with regard</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:268.29pt;left:37.9084pt;z-index:25;letter-spacing:.009em;">to two useful applications: human identiﬁcation (ID) recognition</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:278.26pt;left:37.9084pt;z-index:26;letter-spacing:.01em;">and gender recognition. More than 500 different experiments on</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:288.22pt;left:37.9084pt;z-index:27;letter-spacing:.018em;">human ID and gender recognition are carried out under a wide</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:298.18pt;left:37.9084pt;z-index:28;letter-spacing:.001em;">range of circumstances. The effectiveness of the seven human gait</div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:308.15pt;left:37.9084pt;z-index:29;">components for ID and gender recognition is analyzed.</div></span><span class="fmt-4" style="white-space:pre;"><div style="position:absolute;top:324.09pt;left:47.8719pt;z-index:31;letter-spacing:-.005em;">Index   Terms<span class="fmt-5">—Biometrics,   gender   recognition,   human   gait</span></div></span><span class="fmt-5" style="white-space:pre;"><div style="position:absolute;top:334.05pt;left:37.9084pt;z-index:32;letter-spacing:-.001em;">recognition, visual surveillance.</div></span><span style="white-space:pre;"><div style="position:absolute;top:363.95pt;left:127.52pt;z-index:34;letter-spacing:.001em;">I.  I<span class="fmt-7">NTRODUCTION</span></div></span><span class="fmt-8" style="white-space:pre;"><div style="position:absolute;top:377.44pt;left:37.908pt;z-index:35;">C</div></span><span style="white-space:pre;"><div style="position:absolute;top:404.79pt;left:37.9085pt;z-index:38;">toring systems. In these applications, human beings are usually</div></span><span style="white-space:pre;"><div style="position:absolute;top:380.87pt;left:61.479pt;z-index:36;letter-spacing:-.007em;">OMMERCIAL visual surveillance has many applications,</div></span><span style="white-space:pre;"><div style="position:absolute;top:392.84pt;left:61.479pt;z-index:37;letter-spacing:.012em;">e.g., in public transportation, banks, and car park moni-</div></span><span style="white-space:pre;"><div style="position:absolute;top:416.75pt;left:37.9085pt;z-index:39;letter-spacing:.006em;">among the main foci of attention. Therefore some kind of bio-</div></span><span style="white-space:pre;"><div style="position:absolute;top:428.7pt;left:37.9085pt;z-index:40;letter-spacing:.001em;">metric information [20], [32] should be extracted from surveil-</div></span><span style="white-space:pre;"><div style="position:absolute;top:440.65pt;left:37.9085pt;z-index:41;letter-spacing:-.009em;">lance  sequences  to  help  in  the  classiﬁcation  and  analysis  of</div></span><span style="white-space:pre;"><div style="position:absolute;top:452.61pt;left:37.9085pt;z-index:42;letter-spacing:-.008em;">behavior.</div></span><span style="white-space:pre;"><div style="position:absolute;top:464.57pt;left:47.8711pt;z-index:43;letter-spacing:-.01em;">Previously,  biometric  research  has  concentrated  on  human</div></span><span style="white-space:pre;"><div style="position:absolute;top:476.52pt;left:37.9085pt;z-index:44;letter-spacing:.006em;">authentication and authorization, utilizing face images, ﬁnger-</div></span><span style="white-space:pre;"><div style="position:absolute;top:488.48pt;left:37.9085pt;z-index:45;letter-spacing:-.003em;">prints,  palm  prints,  shoeprint,  iris  images,  and  handwriting,</div></span><span style="white-space:pre;"><div style="position:absolute;top:500.43pt;left:37.9085pt;z-index:46;letter-spacing:.004em;">but these conventional biometric resources suffer from several</div></span><span style="white-space:pre;"><div style="position:absolute;top:512.38pt;left:37.9085pt;z-index:47;letter-spacing:.001em;">limitations.</div></span><span style="white-space:pre;"><div style="position:absolute;top:524.34pt;left:47.0432pt;z-index:48;letter-spacing:-.001em;">1)  Distance between camera/scanner and people: At present,</div></span><span style="white-space:pre;"><div style="position:absolute;top:536.3pt;left:60.8225pt;z-index:49;letter-spacing:.007em;">none of the aforementioned conventional biometrics can</div></span><span style="white-space:pre;"><div style="position:absolute;top:548.25pt;left:60.8225pt;z-index:50;letter-spacing:.017em;">work well from a large distance. In visual surveillance,</div></span><span style="white-space:pre;"><div style="position:absolute;top:560.21pt;left:60.8225pt;z-index:51;letter-spacing:.011em;">the distances between the cameras and the people under</div></span><span style="white-space:pre;"><div style="position:absolute;top:572.16pt;left:60.8225pt;z-index:52;letter-spacing:-.006em;">surveillance are often large. The camera may be set on the</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:604.27pt;left:46.377pt;z-index:53;letter-spacing:-.006em;">Manuscript received January 21, 2007; revised May 25, 2007 and August 25,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:613.24pt;left:37.908pt;z-index:54;letter-spacing:-.001em;">2007. This paper was recommended by Associate Editor Z. Wang.</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:622.2pt;left:45.8821pt;z-index:55;letter-spacing:.013em;">X. Li and S. J. Maybank are with the School of Computer Science and In-</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:631.17pt;left:37.908pt;z-index:56;letter-spacing:.01em;">formation Systems, Brikbeck College, University of London, London WC1E</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:640.14pt;left:37.908pt;z-index:57;">7HX, U.K. (e-mail: xuelong@dcs.bbk.ac.uk; sjmaybank@dcs.bbk.ac.uk).</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:649.1pt;left:45.8821pt;z-index:58;letter-spacing:.002em;">S. Yan is with the Department of Electrical and Computer Engineering, Na-</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:658.07pt;left:37.908pt;z-index:59;">tional University of Singapore, Singapore 117576.</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:667.03pt;left:45.8821pt;z-index:60;letter-spacing:.014em;">D. Tao is with the Biometric Research Center, Department of Computing,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:676pt;left:37.908pt;z-index:61;letter-spacing:.003em;">Hong Kong Polytechnic University, Kowloon, Hong Kong, China (e-mail: cs-</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:684.97pt;left:37.908pt;z-index:62;">dct@comp.polyu.edu.hk).</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:693.93pt;left:45.8821pt;z-index:63;letter-spacing:.001em;">D. Xu is with the School of Computer Engineering, Nanyang Technological</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:702.9pt;left:37.908pt;z-index:64;letter-spacing:-.001em;">University, Singpore Blk N4, Singapore (e-mail: dongxu@ntu.edu.sg).</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:711.87pt;left:45.8821pt;z-index:65;letter-spacing:-.004em;">Color versions of one or more of the ﬁgures in this paper are available online</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:720.83pt;left:37.908pt;z-index:66;letter-spacing:-.001em;">at http://ieeexplore.ieee.org.</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:729.8pt;left:45.8821pt;z-index:67;">Digital Object Identiﬁer 10.1109/TSMCC.2007.913886</div></span><span class="fmt-3" style="white-space:pre;"><div style="position:absolute;top:121.46pt;left:364.65pt;z-index:8;letter-spacing:-.005em;">, Senior Member, IEEE<span class="fmt-2">,</span></div></span><span class="fmt-2" style="white-space:pre;"><div style="position:absolute;top:134.41pt;left:362.42pt;z-index:14;letter-spacing:-.004em;">, and Dong Xu<span class="fmt-3">, Member, IEEE</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:177.74pt;left:323.84pt;z-index:68;">top of a building, in the ceiling of a corridor, or wherever</div></span><span style="white-space:pre;"><div style="position:absolute;top:189.69pt;left:323.84pt;z-index:69;letter-spacing:-.004em;">a panorama of the surveyed area can be obtained. In these</div></span><span style="white-space:pre;"><div style="position:absolute;top:201.65pt;left:323.84pt;z-index:70;letter-spacing:.009em;">situations, it is almost impossible to acquire the detailed</div></span><span style="white-space:pre;"><div style="position:absolute;top:213.6pt;left:323.84pt;z-index:71;letter-spacing:-.001em;">conventional biometric information.</div></span><span style="white-space:pre;"><div style="position:absolute;top:225.55pt;left:310.06pt;z-index:72;letter-spacing:.01em;">2)  People (user) cooperation: This is often required to cap-</div></span><span style="white-space:pre;"><div style="position:absolute;top:237.51pt;left:323.84pt;z-index:73;letter-spacing:.008em;">ture conventional biometric information. For example, a</div></span><span style="white-space:pre;"><div style="position:absolute;top:249.47pt;left:323.84pt;z-index:74;letter-spacing:-.001em;">scanner may be provided to obtain a ﬁngerprint or a palm</div></span><span style="white-space:pre;"><div style="position:absolute;top:261.43pt;left:323.84pt;z-index:75;letter-spacing:-.011em;">print. The scanner must be used in the correct way. The re-</div></span><span style="white-space:pre;"><div style="position:absolute;top:273.38pt;left:323.84pt;z-index:76;letter-spacing:-.002em;">quirements are even stricter for iris recognition because it</div></span><span style="white-space:pre;"><div style="position:absolute;top:285.33pt;left:323.84pt;z-index:77;letter-spacing:-.004em;">is necessary to look through an eyepiece. Most of the cur-</div></span><span style="white-space:pre;"><div style="position:absolute;top:297.29pt;left:323.84pt;z-index:78;letter-spacing:.005em;">rent face databases have been built with the participants’</div></span><span style="white-space:pre;"><div style="position:absolute;top:309.24pt;left:323.84pt;z-index:79;letter-spacing:.008em;">cooperation. For these applications, people are normally</div></span><span style="white-space:pre;"><div style="position:absolute;top:321.2pt;left:323.84pt;z-index:80;">the users of the system.</div></span><span style="white-space:pre;"><div style="position:absolute;top:333.16pt;left:310.06pt;z-index:81;letter-spacing:.004em;">3)  People’s (user) attention in authentication and authoriza-</div></span><span style="white-space:pre;"><div style="position:absolute;top:345.11pt;left:323.84pt;z-index:82;letter-spacing:-.002em;">tion: Previous work on conventional biometrics resources</div></span><span style="white-space:pre;"><div style="position:absolute;top:357.06pt;left:323.84pt;z-index:83;letter-spacing:-.011em;">has focussed on the recognition rate, but paid less attention</div></span><span style="white-space:pre;"><div style="position:absolute;top:369.02pt;left:323.84pt;z-index:84;letter-spacing:.005em;">to other aspects, e.g., manners. For instance, it might not</div></span><span style="white-space:pre;"><div style="position:absolute;top:380.98pt;left:323.84pt;z-index:85;letter-spacing:.009em;">be polite to ask a person to stare at a camera that is used</div></span><span style="white-space:pre;"><div style="position:absolute;top:392.93pt;left:323.84pt;z-index:86;letter-spacing:-.006em;">to capture iris patterns. Even worse, when something goes</div></span><span style="white-space:pre;"><div style="position:absolute;top:404.89pt;left:323.84pt;z-index:87;letter-spacing:-.009em;">wrong with the system, the person has to repeat the action.</div></span><span style="white-space:pre;"><div style="position:absolute;top:416.84pt;left:310.89pt;z-index:88;letter-spacing:.002em;">For visual surveillance applications, the earlier conventional</div></span><span style="white-space:pre;"><div style="position:absolute;top:428.79pt;left:300.92pt;z-index:89;letter-spacing:-.004em;">biometrics resources are difﬁcult to utilize, and human gait pro-</div></span><span style="white-space:pre;"><div style="position:absolute;top:440.75pt;left:300.92pt;z-index:90;letter-spacing:-.01em;">vides  an  interesting  alternative.  A  gait  describes  the  manner</div></span><span style="white-space:pre;"><div style="position:absolute;top:452.71pt;left:300.92pt;z-index:91;letter-spacing:.015em;">of a person’s walking. It can be acquired at a distance, and if</div></span><span style="white-space:pre;"><div style="position:absolute;top:464.66pt;left:300.92pt;z-index:92;letter-spacing:-.002em;">necessary, without the walker’s cooperation or knowledge.</div></span><span style="white-space:pre;"><div style="position:absolute;top:476.62pt;left:310.89pt;z-index:93;letter-spacing:.017em;">Human gait information was analyzed ﬁrst in surgery [29]</div></span><span style="white-space:pre;"><div style="position:absolute;top:488.57pt;left:300.92pt;z-index:94;letter-spacing:.006em;">and psychology [11], [18]. In surgical applications, a patient’s</div></span><span style="white-space:pre;"><div style="position:absolute;top:500.53pt;left:300.92pt;z-index:95;letter-spacing:-.014em;">gait  pattern  was  a  basis  for  choosing  an  appropriate  medical</div></span><span style="white-space:pre;"><div style="position:absolute;top:512.48pt;left:300.92pt;z-index:96;letter-spacing:.01em;">treatment (in this case, the data were gained at a close range).</div></span><span style="white-space:pre;"><div style="position:absolute;top:524.44pt;left:300.92pt;z-index:97;letter-spacing:-.011em;">Murray [29] classiﬁed pathologically abnormal gaits into several</div></span><span style="white-space:pre;"><div style="position:absolute;top:536.4pt;left:300.92pt;z-index:98;letter-spacing:-.001em;">groups for suitable treatments. This classiﬁcation was achieved</div></span><span style="white-space:pre;"><div style="position:absolute;top:548.35pt;left:300.92pt;z-index:99;letter-spacing:-.002em;">by comparing a patients’ gait patterns with normal gait patterns</div></span><span style="white-space:pre;"><div style="position:absolute;top:560.3pt;left:300.92pt;z-index:100;letter-spacing:.006em;">obtained from a control group. Johansson reported in [18] that</div></span><span style="white-space:pre;"><div style="position:absolute;top:572.26pt;left:300.92pt;z-index:101;letter-spacing:-.001em;">walking subjects could be recognized on the basis of their gaits</div></span><span style="white-space:pre;"><div style="position:absolute;top:584.22pt;left:300.92pt;z-index:102;letter-spacing:.011em;">alone. In the scope of computer science, some early efforts to</div></span><span style="white-space:pre;"><div style="position:absolute;top:596.17pt;left:300.92pt;z-index:103;">recognize a person by gait are introduced in [5], [23], and [30].</div></span><span style="white-space:pre;"><div style="position:absolute;top:608.13pt;left:300.92pt;z-index:104;letter-spacing:-.015em;">Many  other  efforts  have  also  been  spent  on  gait  information</div></span><span style="white-space:pre;"><div style="position:absolute;top:620.08pt;left:300.92pt;z-index:105;letter-spacing:-.01em;">processing  [2],  [4],  [6],  [9],  [10],  [12],  [19],  [22],  [38],  and</div></span><span style="white-space:pre;"><div style="position:absolute;top:632.03pt;left:300.92pt;z-index:106;letter-spacing:-.001em;">since then, gait silhouette information has been widely utilized,</div></span><span style="white-space:pre;"><div style="position:absolute;top:643.99pt;left:300.92pt;z-index:107;letter-spacing:.011em;">e.g., in [8], [14], [21], [37], and [39]. Human gait recognition</div></span><span style="white-space:pre;"><div style="position:absolute;top:655.95pt;left:300.92pt;z-index:108;letter-spacing:.002em;">(HGR) has applications not only in visual surveillance but also</div></span><span style="white-space:pre;"><div style="position:absolute;top:667.9pt;left:300.92pt;z-index:109;letter-spacing:.008em;">in human computer interaction, access control, human motion</div></span><span style="white-space:pre;"><div style="position:absolute;top:679.86pt;left:300.92pt;z-index:110;letter-spacing:-.011em;">analysis [1], [7], and identiﬁcation [32]. Some important surveys</div></span><span style="white-space:pre;"><div style="position:absolute;top:691.81pt;left:300.92pt;z-index:111;letter-spacing:.002em;">along with detailed comparisons of different algorithms can be</div></span><span style="white-space:pre;"><div style="position:absolute;top:703.77pt;left:300.92pt;z-index:112;">found in [1], [13], [16], and [27].</div></span><span style="white-space:pre;"><div style="position:absolute;top:715.72pt;left:310.89pt;z-index:113;letter-spacing:.016em;">Human gait is affected by certain factors, including physi-</div></span><span style="white-space:pre;"><div style="position:absolute;top:727.68pt;left:300.92pt;z-index:114;letter-spacing:-.004em;">cal characteristics of people and environmental factors, such as:</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 2</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-6"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:42.12pt;z-index:1;">146</div><div style="position:absolute;top:34.3462pt;left:128.36pt;z-index:1;letter-spacing:-.003em;">IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART C: APPLICATIONS AND REVIEWS, VOL. 38, NO. 2, MARCH 2008</div></span><span style="white-space:pre;"><div style="position:absolute;top:65.7572pt;left:42.12pt;z-index:2;letter-spacing:.009em;">1) camera factors, such as viewpoint [31], affect the measure-</div></span><span style="white-space:pre;"><div style="position:absolute;top:77.7094pt;left:42.12pt;z-index:3;letter-spacing:.009em;">ment of the gait (and do not normally affect the gait itself); 2)</div></span><span style="white-space:pre;"><div style="position:absolute;top:89.6705pt;left:42.12pt;z-index:4;letter-spacing:-.005em;">time elapsed [31], [34] is also a key issue in visual surveillance;</div></span><span style="white-space:pre;"><div style="position:absolute;top:101.62pt;left:42.12pt;z-index:5;letter-spacing:-.004em;">3) the walking ﬁgure’s carrying status [35] and clothing such as</div></span><span style="white-space:pre;"><div style="position:absolute;top:113.57pt;left:42.12pt;z-index:6;letter-spacing:.005em;">clothes and shoes [31]; 4) kinematics features such as walking</div></span><span style="white-space:pre;"><div style="position:absolute;top:125.54pt;left:42.12pt;z-index:7;letter-spacing:-.009em;">speed [33], bounciness, and rhythm [24], note that qualities such</div></span><span style="white-space:pre;"><div style="position:absolute;top:137.49pt;left:42.12pt;z-index:8;letter-spacing:.01em;">as bounciness and rhythm are properties of the gait itself; and</div></span><span style="white-space:pre;"><div style="position:absolute;top:149.45pt;left:42.12pt;z-index:9;letter-spacing:-.006em;">5) other factors, such as injury, disguise, image quality [25], ob-</div></span><span style="white-space:pre;"><div style="position:absolute;top:161.4pt;left:42.12pt;z-index:10;letter-spacing:.002em;">server’ familiarity with the people under surveillance, lighting,</div></span><span style="white-space:pre;"><div style="position:absolute;top:173.35pt;left:42.12pt;z-index:11;letter-spacing:.012em;">background, and walking surface. The human ID dataset [32]</div></span><span style="white-space:pre;"><div style="position:absolute;top:185.31pt;left:42.12pt;z-index:12;letter-spacing:-.01em;">studied several of these factors and provided baseline algorithms</div></span><span style="white-space:pre;"><div style="position:absolute;top:197.27pt;left:42.12pt;z-index:13;letter-spacing:-.003em;">for researchers. So far, how to measure the effect of each factor</div></span><span style="white-space:pre;"><div style="position:absolute;top:209.22pt;left:42.12pt;z-index:14;">or combination of factors is still an open issue.</div></span><span style="white-space:pre;"><div style="position:absolute;top:221.18pt;left:52.0826pt;z-index:15;letter-spacing:.002em;">Any one factor may be correlated with other factors. For ex-</div></span><span style="white-space:pre;"><div style="position:absolute;top:233.13pt;left:42.12pt;z-index:16;letter-spacing:.003em;">ample, a change in a walking surface or a shoe type may cause</div></span><span style="white-space:pre;"><div style="position:absolute;top:245.08pt;left:42.12pt;z-index:17;letter-spacing:.001em;">a change in speed. These correlations between different factors</div></span><span style="white-space:pre;"><div style="position:absolute;top:257.05pt;left:42.12pt;z-index:18;letter-spacing:.013em;">can be very signiﬁcant because the different components of a</div></span><span style="white-space:pre;"><div style="position:absolute;top:269pt;left:42.12pt;z-index:19;letter-spacing:.013em;">person’s gait have wide variations as compared with conven-</div></span><span style="white-space:pre;"><div style="position:absolute;top:280.95pt;left:42.12pt;z-index:20;letter-spacing:.013em;">tional biometrics, e.g., a person’s hand could touch his leg or</div></span><span style="white-space:pre;"><div style="position:absolute;top:292.91pt;left:42.12pt;z-index:21;letter-spacing:.002em;">head. These large variations are not found in conventional bio-</div></span><span style="white-space:pre;"><div style="position:absolute;top:304.86pt;left:42.12pt;z-index:22;letter-spacing:-.009em;">metrics, e.g., we cannot move the eyes below the nose—in other</div></span><span style="white-space:pre;"><div style="position:absolute;top:316.82pt;left:42.12pt;z-index:23;letter-spacing:-.01em;">words, the relative positions of the eyes and nose on the face can</div></span><span style="white-space:pre;"><div style="position:absolute;top:328.78pt;left:42.12pt;z-index:24;letter-spacing:-.003em;">vary only over a small range. The larger variations in gait make</div></span><span style="white-space:pre;"><div style="position:absolute;top:340.73pt;left:42.12pt;z-index:25;letter-spacing:-.006em;">the gait difﬁcult to measure, but at the same time, indicate that a</div></span><span style="white-space:pre;"><div style="position:absolute;top:352.68pt;left:42.12pt;z-index:26;letter-spacing:.005em;">large amount of information might be recovered from the gait.</div></span><span style="white-space:pre;"><div style="position:absolute;top:364.64pt;left:42.12pt;z-index:27;letter-spacing:-.002em;">From the aforementioned examples, we can understand that the</div></span><span style="white-space:pre;"><div style="position:absolute;top:376.59pt;left:42.12pt;z-index:28;letter-spacing:.007em;">correlations between the different gait factors/components are</div></span><span style="white-space:pre;"><div style="position:absolute;top:388.55pt;left:42.12pt;z-index:29;">very signiﬁcant for human recognition and gender recognition.</div></span><span style="white-space:pre;"><div style="position:absolute;top:400.51pt;left:52.0826pt;z-index:30;letter-spacing:.015em;">In this paper, we ﬁrst separate human gait into seven com-</div></span><span style="white-space:pre;"><div style="position:absolute;top:412.46pt;left:42.12pt;z-index:31;letter-spacing:.01em;">ponents, and then, perform human gait recognition ﬁrst based</div></span><span style="white-space:pre;"><div style="position:absolute;top:424.42pt;left:42.12pt;z-index:32;letter-spacing:.012em;">on the entire human gait silhouette, then on each of the seven</div></span><span style="white-space:pre;"><div style="position:absolute;top:436.37pt;left:42.12pt;z-index:33;letter-spacing:.008em;">components individually, and ﬁnally, on certain combinations</div></span><span style="white-space:pre;"><div style="position:absolute;top:448.32pt;left:42.12pt;z-index:34;letter-spacing:-.002em;">of the seven components. The results show that the gait is help-</div></span><span style="white-space:pre;"><div style="position:absolute;top:460.29pt;left:42.12pt;z-index:35;letter-spacing:-.006em;">ful for efﬁciently and effectively recognizing people. Moreover,</div></span><span style="white-space:pre;"><div style="position:absolute;top:472.24pt;left:42.12pt;z-index:36;letter-spacing:.001em;">human gait not only shows the distinctive moving silhouette of</div></span><span style="white-space:pre;"><div style="position:absolute;top:484.19pt;left:42.12pt;z-index:37;letter-spacing:.006em;">a human body, but also reﬂects the walker’s physical situation</div></span><span style="white-space:pre;"><div style="position:absolute;top:496.15pt;left:42.12pt;z-index:38;letter-spacing:-.011em;">and even his or her psychological state. Therefore, after studying</div></span><span style="white-space:pre;"><div style="position:absolute;top:508.1pt;left:42.12pt;z-index:39;letter-spacing:-.01em;">different components of the human silhouette and their contribu-</div></span><span style="white-space:pre;"><div style="position:absolute;top:520.05pt;left:42.12pt;z-index:40;letter-spacing:-.007em;">tion to human gait recognition, we also studied the effectiveness</div></span><span style="white-space:pre;"><div style="position:absolute;top:532.02pt;left:42.12pt;z-index:41;letter-spacing:.005em;">of these components for human gender recognition, which has</div></span><span style="white-space:pre;"><div style="position:absolute;top:543.97pt;left:42.12pt;z-index:42;">useful applications in commercial visual surveillance.</div></span><span style="white-space:pre;"><div style="position:absolute;top:555.92pt;left:52.0826pt;z-index:43;letter-spacing:.012em;">The rest of the paper is organized as follows. Section II in-</div></span><span style="white-space:pre;"><div style="position:absolute;top:567.88pt;left:42.12pt;z-index:44;letter-spacing:.017em;">troduces the human gait modeling scheme, and describes the</div></span><span style="white-space:pre;"><div style="position:absolute;top:579.83pt;left:42.12pt;z-index:45;letter-spacing:.01em;">segmentation of the human silhouette into seven components.</div></span><span style="white-space:pre;"><div style="position:absolute;top:591.79pt;left:42.12pt;z-index:46;letter-spacing:.008em;">Section III introduces the dataset utilized for the experiments.</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.75pt;left:42.12pt;z-index:47;letter-spacing:-.005em;">The use of gait for human recognition and gender recognition is</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.7pt;left:42.12pt;z-index:48;letter-spacing:.001em;">studied in Sections IV and V, respectively. Finally, the conclu-</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.66pt;left:42.12pt;z-index:49;">sion is drawn in Section VI.</div></span><span style="white-space:pre;"><div style="position:absolute;top:658.59pt;left:109.72pt;z-index:55;letter-spacing:.002em;">II.  H<span class="fmt-7">UMAN</span> <span>G</span><span class="fmt-7">AIT</span> <span>M</span><span class="fmt-7">ODELING</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:52.083pt;z-index:56;letter-spacing:.001em;">We segment the averaged gait image into seven components</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:42.1204pt;z-index:57;letter-spacing:-.002em;">according  to  the  usually  segmentation  method  as  shown  in</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:42.1204pt;z-index:58;letter-spacing:-.005em;">Fig. 1.The ﬁrst component is the head; the second component is</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:42.1204pt;z-index:59;letter-spacing:.003em;">the arm (it also includes the breast); the third component is the</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:42.1204pt;z-index:60;">trunk (different from traditional “trunk,” the trunk in this paper</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.29pt;left:42.1204pt;z-index:61;letter-spacing:.007em;">excludes the breast); the fourth component is the thigh (it also</div></span><img style="position:absolute;left:309.11pt;top:68.266pt;width:243.1pt;height:151.19pt;z-index:62;" src="./target/cc9fcac082d8ba7131ee289b86242cbd.png" alt="Image_37_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:230.37pt;left:305.14pt;z-index:63;letter-spacing:-.007em;">Fig.  1.</div><div style="position:absolute;top:230.37pt;left:336.2pt;z-index:63;letter-spacing:-.006em;">Three-sided  human  view  (adapted  from  http://www.vbﬂorida.com/</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:239.33pt;left:305.14pt;z-index:64;letter-spacing:.001em;">htmassage/clientinfoform.cfm with minor changes).</div></span><img style="position:absolute;left:360.1pt;top:270.06pt;width:141.1pt;height:86.3892pt;z-index:65;" src="./target/a0fd92b46eb492f9491720017f30c88c.png" alt="Image_80_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:367.37pt;left:305.14pt;z-index:66;letter-spacing:-.004em;">Fig. 2.</div><div style="position:absolute;top:367.37pt;left:334.45pt;z-index:66;letter-spacing:-.005em;">Averaged human gait image partition model (the bottom component is</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:376.33pt;left:305.14pt;z-index:67;">part 7).</div></span><span style="white-space:pre;"><div style="position:absolute;top:412.51pt;left:305.14pt;z-index:68;letter-spacing:.003em;">includes the hip); the ﬁfth component is the front leg; the sixth</div></span><span style="white-space:pre;"><div style="position:absolute;top:424.46pt;left:305.14pt;z-index:69;letter-spacing:.016em;">component is the back leg; and the seventh component is the</div></span><span style="white-space:pre;"><div style="position:absolute;top:436.41pt;left:305.14pt;z-index:70;letter-spacing:.007em;">feet. The front-leg and back-leg are included as separate com-</div></span><span style="white-space:pre;"><div style="position:absolute;top:448.37pt;left:305.14pt;z-index:71;letter-spacing:.005em;">ponents because of the bipedal walking style. During walking,</div></span><span style="white-space:pre;"><div style="position:absolute;top:460.33pt;left:305.14pt;z-index:72;letter-spacing:.01em;">the left-leg and the right-leg come to front/back by turns. The</div></span><span style="white-space:pre;"><div style="position:absolute;top:472.29pt;left:305.14pt;z-index:73;">resulting partition is shown in Fig. 2.</div></span><span style="white-space:pre;"><div style="position:absolute;top:484.24pt;left:315.1pt;z-index:74;letter-spacing:-.008em;">The averaged gait image partition model is constructed by the</div></span><span style="white-space:pre;"><div style="position:absolute;top:496.19pt;left:305.14pt;z-index:75;letter-spacing:-.001em;">following steps.</div></span><span style="white-space:pre;"><div style="position:absolute;top:508.15pt;left:314.27pt;z-index:76;letter-spacing:-.014em;">1)  Calculate the mean image of all averaged gait images in the</div></span><span style="white-space:pre;"><div style="position:absolute;top:520.11pt;left:328.05pt;z-index:79;">gallery set (Fig. 4 shows sample images for <span class="fmt-9">average gait</span>;</div></span><span style="white-space:pre;"><div style="position:absolute;top:532.06pt;left:328.05pt;z-index:80;letter-spacing:-.008em;">Table I describes probe and gallery sets, and Section IV-A</div></span><span style="white-space:pre;"><div style="position:absolute;top:544.02pt;left:328.05pt;z-index:81;letter-spacing:-.001em;">describes how to obtain the averaged gait image) [32].</div></span><span style="white-space:pre;"><div style="position:absolute;top:555.97pt;left:314.27pt;z-index:82;letter-spacing:.018em;">2)  Select six control points according to Fig. 2. Here, two</div></span><span style="white-space:pre;"><div style="position:absolute;top:567.92pt;left:328.05pt;z-index:83;">points are marked on the mean image of all averaged gait</div></span><span style="white-space:pre;"><div style="position:absolute;top:579.88pt;left:328.05pt;z-index:84;letter-spacing:.02em;">images in the gallery to locate the head; two points are</div></span><span style="white-space:pre;"><div style="position:absolute;top:591.84pt;left:328.05pt;z-index:85;letter-spacing:-.006em;">marked to locate the arm (including breast) and trunk; one</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.79pt;left:328.05pt;z-index:86;letter-spacing:.011em;">point is marked to indicate the thigh (with hip); and one</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.75pt;left:328.05pt;z-index:87;letter-spacing:-.003em;">point is marked to indicate the feet. The six control points</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.7pt;left:328.05pt;z-index:88;">are shown in the left subﬁgure of Fig. 2.</div></span><span style="white-space:pre;"><div style="position:absolute;top:639.65pt;left:314.27pt;z-index:89;letter-spacing:-.01em;">3)  Use lines to connect the relevant pairs of points to partition</div></span><span style="white-space:pre;"><div style="position:absolute;top:651.61pt;left:328.05pt;z-index:90;letter-spacing:-.011em;">the mean image into seven parts. The connections between</div></span><span style="white-space:pre;"><div style="position:absolute;top:663.57pt;left:328.05pt;z-index:91;letter-spacing:-.008em;">the points are shown in the left subﬁgure of Fig. 2, and the</div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:328.05pt;z-index:92;letter-spacing:.007em;">resulting segmentation is shown in the right subﬁgure of</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:328.05pt;z-index:93;">Fig. 2.</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:315.1pt;z-index:94;letter-spacing:-.007em;">Fig. 3 the seven templates are shown in white. The subﬁgures</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:305.14pt;z-index:95;letter-spacing:-.011em;">from left to right can extract head, arm (with breast), trunk, thigh</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.35pt;left:305.14pt;z-index:96;letter-spacing:.013em;">(with hip), front leg, back leg, and feet, respectively, from an</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:305.14pt;z-index:97;letter-spacing:-.001em;">averaged gait image.</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 3</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-6"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:60.1753pt;z-index:3;letter-spacing:-.002em;">: GAIT COMPONENTS AND THEIR APPLICATION TO GENDER RECOGNITION</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:65.7237pt;left:280.15pt;z-index:5;letter-spacing:-.026em;">TABLE I</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:292.44pt;z-index:18;letter-spacing:.001em;">W</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:37.908pt;z-index:2;">LI <span class="fmt-10">et al.</span></div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:109.67pt;z-index:17;">T<span class="fmt-11">WELVE</span> <span>P</span><span class="fmt-11">ROBE</span> <span>S</span><span class="fmt-11">ETS FOR</span> <span>C</span><span class="fmt-11">HALLENGE</span> <span>E</span><span class="fmt-11">XPERIMENTS</span><span>,</span> <span class="fmt-11">IN</span></div></span><img style="position:absolute;left:40.437pt;top:234.76pt;width:246pt;height:49.92pt;z-index:29;" src="./target/2e1048a5e499859e406ca66ed809d2e9.png" alt="Image_23_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:295.59pt;left:37.908pt;z-index:30;letter-spacing:.002em;">Fig. 3.</div><div style="position:absolute;top:295.59pt;left:67.5647pt;z-index:30;letter-spacing:.001em;">Seven templates are shown in white. The subﬁgures from left to right</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:304.56pt;left:37.908pt;z-index:31;letter-spacing:.001em;">can extract head, arm (with breast), trunk, thigh (with hip), front-leg, back-leg,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:313.52pt;left:37.908pt;z-index:32;letter-spacing:-.004em;">and feet, respectively, from an AGI.</div></span><span style="white-space:pre;"><div style="position:absolute;top:344.16pt;left:47.871pt;z-index:33;letter-spacing:-.012em;">The  segments  shown  in  the  right  subﬁgure  of  Fig.  2  cor-</div></span><span style="white-space:pre;"><div style="position:absolute;top:356.12pt;left:37.9084pt;z-index:34;letter-spacing:.017em;">respond to the seven templates in Fig. 3. The image segment</div></span><span style="white-space:pre;"><div style="position:absolute;top:368.08pt;left:37.9084pt;z-index:35;letter-spacing:-.007em;">corresponding to a given template is extracted from an averaged</div></span><span style="white-space:pre;"><div style="position:absolute;top:380.04pt;left:37.9084pt;z-index:36;letter-spacing:.019em;">gait image by placing the template on the gait image and se-</div></span><span style="white-space:pre;"><div style="position:absolute;top:391.99pt;left:37.9084pt;z-index:37;letter-spacing:-.004em;">lecting all the pixels in the gait image corresponding to the high</div></span><span style="white-space:pre;"><div style="position:absolute;top:403.94pt;left:37.9084pt;z-index:38;letter-spacing:.004em;">values in the template. By this means, the complement to a se-</div></span><span style="white-space:pre;"><div style="position:absolute;top:415.9pt;left:37.9084pt;z-index:39;letter-spacing:-.004em;">lected component can also be obtained. The subﬁgures in Fig. 3</div></span><span style="white-space:pre;"><div style="position:absolute;top:427.85pt;left:37.9084pt;z-index:40;letter-spacing:-.003em;">from left to right correspond to seven components shown in the</div></span><span style="white-space:pre;"><div style="position:absolute;top:439.81pt;left:37.9084pt;z-index:41;letter-spacing:-.01em;">right subﬁgure in Fig. 2. They are head, arm (with breast), trunk,</div></span><span style="white-space:pre;"><div style="position:absolute;top:451.77pt;left:37.9084pt;z-index:42;letter-spacing:-.002em;">thigh (with hip), front-leg, back-leg, and feet, respectively.</div></span><span style="white-space:pre;"><div style="position:absolute;top:479.25pt;left:65.8176pt;z-index:50;letter-spacing:-.001em;">III.  B<span class="fmt-7">RIEF</span> <span>R</span><span class="fmt-7">EVIEW OF THE</span> <span>USF G</span><span class="fmt-7">AIT</span> <span>D</span><span class="fmt-7">ATABASE</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:496.19pt;left:47.871pt;z-index:51;letter-spacing:.005em;">The experimental images were taken from the University of</div></span><span style="white-space:pre;"><div style="position:absolute;top:508.15pt;left:37.9084pt;z-index:52;letter-spacing:.003em;">South Florida (USF) HumanID outdoor gait (people–walking–</div></span><span style="white-space:pre;"><div style="position:absolute;top:520.1pt;left:37.9084pt;z-index:53;letter-spacing:.011em;">sequence) database that has been built and widely utilized for</div></span><span style="white-space:pre;"><div style="position:absolute;top:532.06pt;left:37.9084pt;z-index:54;letter-spacing:-.011em;">vision-based gait recognition. It consists of 1870 sequences from</div></span><span style="white-space:pre;"><div style="position:absolute;top:544.02pt;left:37.9084pt;z-index:55;">122 subjects. For each of the subjects, there exist the following</div></span><span style="white-space:pre;"><div style="position:absolute;top:555.97pt;left:37.9084pt;z-index:56;letter-spacing:.01em;">covariates: change in viewpoint (left or right), change in shoe</div></span><span style="white-space:pre;"><div style="position:absolute;top:567.92pt;left:37.9084pt;z-index:57;letter-spacing:.016em;">type (A or B), change in walking surface (grass or concrete),</div></span><span style="white-space:pre;"><div style="position:absolute;top:579.88pt;left:37.9084pt;z-index:60;letter-spacing:.006em;">change in carrying condition (<span class="fmt-9">carrying</span> a briefcase or no brief-</div></span><span style="white-space:pre;"><div style="position:absolute;top:591.84pt;left:37.9084pt;z-index:61;letter-spacing:-.001em;">case), and change in elapsed time (May or November) between</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.79pt;left:37.9084pt;z-index:62;letter-spacing:.009em;">sequences being compared. All these covariates are important</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.75pt;left:37.9084pt;z-index:63;letter-spacing:.007em;">for different aspects/applications. Probably, the effect of a co-</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.7pt;left:37.9084pt;z-index:64;letter-spacing:-.013em;">variate  is  more  cleanly  captured  by  its  impact  on  the  match</div></span><span style="white-space:pre;"><div style="position:absolute;top:639.65pt;left:37.9084pt;z-index:65;letter-spacing:.017em;">scores, and it can possibly be measured by testing two probe</div></span><span style="white-space:pre;"><div style="position:absolute;top:651.61pt;left:37.9084pt;z-index:66;letter-spacing:-.004em;">sets that differ in only one factor. Herein, the silhouette data are</div></span><span style="white-space:pre;"><div style="position:absolute;top:663.57pt;left:37.9084pt;z-index:67;letter-spacing:.017em;">normalized in USF HumanID, and the alignment has already</div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:37.9084pt;z-index:68;letter-spacing:.001em;">been done.</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:47.871pt;z-index:69;letter-spacing:.012em;">There is a set of 12 predesigned experiments [32] for algo-</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:37.9084pt;z-index:70;">rithm comparison. For classiﬁer training, the database provides</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:37.9084pt;z-index:71;letter-spacing:-.014em;">a gallery, which was collected in May, with the following covari-</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:37.9084pt;z-index:72;letter-spacing:.005em;">ates: grass, shoe type A, right camera, and no briefcase, which</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:37.9084pt;z-index:73;letter-spacing:-.002em;">includes several new subjects in November. The gallery set has</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:541.54pt;z-index:4;letter-spacing:-.001em;">147</div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:76.2068pt;left:299.96pt;z-index:27;letter-spacing:-.008em;">HICH <span class="fmt-7">H, I,</span> AND <span class="fmt-7">J F</span>OCUS ON THE <span class="fmt-12">Car r ying</span><span class="fmt-7">S</span>TATUS <span class="fmt-7">(BF)</span></div></span><img style="position:absolute;left:126.95pt;top:91.653pt;width:336pt;height:128.16pt;z-index:28;" src="./target/0e1f837b3e220ab21349d28a791888b7.png" alt="Image_117_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:417.27pt;left:300.92pt;z-index:75;">Fig. 4.</div><div style="position:absolute;top:417.27pt;left:330.48pt;z-index:75;letter-spacing:-.001em;">AGIs in the gallery set.</div></span><img style="position:absolute;left:336.46pt;top:234.76pt;width:180pt;height:171.6pt;z-index:74;" src="./target/8ced0ceeef20d5c2684279e0ec425a75.png" alt="Image_67_0" /><span style="white-space:pre;"><div style="position:absolute;top:446.34pt;left:300.92pt;z-index:76;letter-spacing:-.002em;">122 individuals, of which 85 are males and 37 are females. The</div></span><span style="white-space:pre;"><div style="position:absolute;top:458.3pt;left:300.92pt;z-index:79;letter-spacing:-.016em;">averaged  gait  image  (with  size  128<span class="fmt-13">×</span>88  and  provided  by  the</div></span><span style="white-space:pre;"><div style="position:absolute;top:470.25pt;left:300.92pt;z-index:80;letter-spacing:-.004em;">USF database) contains the human gait information available to</div></span><span style="white-space:pre;"><div style="position:absolute;top:482.21pt;left:300.92pt;z-index:81;letter-spacing:-.002em;">the classiﬁer. Examples from the gallery set are given in Fig. 4.</div></span><span style="white-space:pre;"><div style="position:absolute;top:494.17pt;left:300.92pt;z-index:82;letter-spacing:.008em;">For algorithm testing, 12 probe sets are constructed according</div></span><span style="white-space:pre;"><div style="position:absolute;top:506.12pt;left:300.92pt;z-index:83;letter-spacing:.003em;">to the 12 predesigned experiments. Detailed information about</div></span><span style="white-space:pre;"><div style="position:absolute;top:518.07pt;left:300.92pt;z-index:84;letter-spacing:-.003em;">the probe sets is given in Table I.</div></span><span style="white-space:pre;"><div style="position:absolute;top:544pt;left:371.03pt;z-index:87;letter-spacing:-.001em;">IV.  E<span class="fmt-7">XPERIMENTS ON</span> <span>HGR</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:560.93pt;left:310.89pt;z-index:88;letter-spacing:.004em;">Human  gait  is  an  important  biometric  for  identiﬁcation.</div></span><span style="white-space:pre;"><div style="position:absolute;top:572.89pt;left:300.93pt;z-index:89;letter-spacing:-.007em;">Current  research  on  gait  recognition  is  usually  based  on  an</div></span><span style="white-space:pre;"><div style="position:absolute;top:584.84pt;left:300.93pt;z-index:90;">averaged  gait  image  or  a  silhouette  sequence,  or  a  motion</div></span><span style="white-space:pre;"><div style="position:absolute;top:596.79pt;left:300.93pt;z-index:91;letter-spacing:.013em;">structure model. The movements of the different components</div></span><span style="white-space:pre;"><div style="position:absolute;top:608.75pt;left:300.93pt;z-index:92;letter-spacing:-.001em;">(e.g., head, arm, and thigh) in the averaged gait image have not</div></span><span style="white-space:pre;"><div style="position:absolute;top:620.71pt;left:300.93pt;z-index:93;letter-spacing:-.003em;">been studied closely.</div></span><span style="white-space:pre;"><div style="position:absolute;top:632.66pt;left:310.89pt;z-index:94;letter-spacing:-.006em;">In  the  following,  the  gait  recognition  algorithm  is  brieﬂy</div></span><span style="white-space:pre;"><div style="position:absolute;top:644.62pt;left:300.93pt;z-index:95;letter-spacing:-.003em;">described. Then, we describe a series of experiments to demon-</div></span><span style="white-space:pre;"><div style="position:absolute;top:656.57pt;left:300.93pt;z-index:96;letter-spacing:-.009em;">strate the impact of each component on gait recognition. Finally,</div></span><span style="white-space:pre;"><div style="position:absolute;top:668.53pt;left:300.93pt;z-index:97;">experimental results are analyzed.</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:694.45pt;left:300.93pt;z-index:98;letter-spacing:.002em;">A.  Human Gait Recognition</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:310.89pt;z-index:99;letter-spacing:.009em;">Fig. 4 shows some examples of average gaits (AGs), which</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:300.93pt;z-index:100;letter-spacing:-.002em;">demonstrate  that  AGs could be  used for gait  recognition  and</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:300.93pt;z-index:101;letter-spacing:.002em;">gender  recognition,  because  different  people  have  different</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 4</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-6"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:42.12pt;z-index:1;">148</div><div style="position:absolute;top:34.3462pt;left:128.36pt;z-index:1;letter-spacing:-.003em;">IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART C: APPLICATIONS AND REVIEWS, VOL. 38, NO. 2, MARCH 2008</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:65.7237pt;left:283.03pt;z-index:2;letter-spacing:-.023em;">TABLE II</div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:76.2068pt;left:276.29pt;z-index:8;letter-spacing:-.003em;">ESULTS FOR</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:192.62pt;z-index:7;letter-spacing:-.003em;">R<span class="fmt-11">ANK</span> <span>1 E</span><span class="fmt-11">XPERIMENTAL</span> <span>R</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:280.38pt;left:42.12pt;z-index:16;letter-spacing:.002em;">AGs. The AG is the mean image (pixel by pixel) of silhouettes</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:292.33pt;left:42.12pt;z-index:18;letter-spacing:.02em;">over a gait cycle <span class="fmt-6">within a sequence of images. As suggested</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:304.28pt;left:42.12pt;z-index:19;letter-spacing:-.013em;">in [26], a sequence of images is partitioned into a series of subse-</div></span><span style="white-space:pre;"><div style="position:absolute;top:316.25pt;left:42.12pt;z-index:23;letter-spacing:.001em;">quences according to the gait period of length <span class="fmt-14">N</span><span class="fmt-15" style="position:relative;left:0pt;top:1pt;">Gait </span><span>. Then, the</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:328.2pt;left:42.1208pt;z-index:24;letter-spacing:.015em;">binary images (silhouettes) within one cycle (a subsequence)</div></span><span style="white-space:pre;"><div style="position:absolute;top:341.77pt;left:42.1208pt;z-index:25;letter-spacing:.011em;">are  averaged  to  yield  AGs  by  means  of</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:342.1pt;left:225.31pt;z-index:26;letter-spacing:.061em;">AG</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:346.16pt;left:240.61pt;z-index:27;letter-spacing:.104em;">i</div></span><span class="fmt-13" style="white-space:pre;"><div style="position:absolute;top:342.1pt;left:243.93pt;z-index:28;letter-spacing:-.222em;">|</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:347.44pt;left:246.69pt;z-index:33;letter-spacing:.104em;">i</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:339.49pt;left:249.79pt;z-index:30;letter-spacing:-.005em;">T /N </div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:347.44pt;left:249.51pt;z-index:34;letter-spacing:-.018em;">= 1</div></span><span class="fmt-17" style="white-space:pre;"><div style="position:absolute;top:342.24pt;left:265.31pt;z-index:31;letter-spacing:.034em;">G ait </div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:342.1pt;left:285.43pt;z-index:35;letter-spacing:.153em;">=</div></span><img style="position:absolute;left:247.9pt;top:340.21pt;width:3pt;height:7pt;z-index:29;" src="./target/923b7e6dd8676c143eda32393f85b27e.png" alt="__rendered_path__29" /><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:42.1214pt;z-index:36;letter-spacing:.014em;">(</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:353.66pt;left:56.511pt;z-index:38;letter-spacing:.021em;">k</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:361.84pt;left:56.5116pt;z-index:46;letter-spacing:.021em;">k</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:353.66pt;left:60.9212pt;z-index:39;letter-spacing:.011em;">= (</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:361.84pt;left:60.9218pt;z-index:47;letter-spacing:.153em;">=</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:353.66pt;left:70.1559pt;z-index:40;letter-spacing:.104em;">i</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:361.84pt;left:67.042pt;z-index:48;letter-spacing:.164em;">iN</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:353.66pt;left:72.9734pt;z-index:41;letter-spacing:.007em;">+ 1)</div></span><span class="fmt-17" style="white-space:pre;"><div style="position:absolute;top:364.6pt;left:76.167pt;z-index:49;letter-spacing:.034em;">G ait </div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:353.66pt;left:86.1685pt;z-index:42;letter-spacing:.104em;">N</div></span><span class="fmt-17" style="white-space:pre;"><div style="position:absolute;top:356.42pt;left:92.475pt;z-index:43;letter-spacing:.034em;">G ait </div></span><span class="fmt-19" style="white-space:pre;"><div style="position:absolute;top:353.66pt;left:106.21pt;z-index:44;letter-spacing:-.055em;">−</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:353.66pt;left:111.64pt;z-index:45;letter-spacing:-.074em;">1</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:117.76pt;z-index:50;letter-spacing:-.019em;">S</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:124.45pt;z-index:51;letter-spacing:.014em;">(</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:128.32pt;z-index:52;letter-spacing:.021em;">k</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:133.82pt;z-index:53;letter-spacing:.015em;">))</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:141.57pt;z-index:54;letter-spacing:-.026em;">/N</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:360.33pt;left:153.36pt;z-index:55;letter-spacing:.009em;">Gait </div></span><span style="white-space:pre;"><div style="position:absolute;top:355.94pt;left:169.37pt;z-index:56;letter-spacing:-.012em;">,  where</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:204.17pt;z-index:57;letter-spacing:-.019em;">S</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:210.86pt;z-index:58;letter-spacing:.014em;">(</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:214.73pt;z-index:59;letter-spacing:.021em;">k</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:356.27pt;left:220.23pt;z-index:60;letter-spacing:.014em;">)</div></span><span style="white-space:pre;"><div style="position:absolute;top:355.94pt;left:228.1pt;z-index:61;letter-spacing:-.012em;">stands  for  the</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:355.94pt;left:288.76pt;z-index:62;letter-spacing:.001em;">k</div></span><img style="position:absolute;left:279.21pt;top:340.21pt;width:3pt;height:7pt;z-index:32;" src="./target/2e9de5e2a89a554e5316a9a4f1b0472d.png" alt="__rendered_path__32" /><span style="white-space:pre;"><div style="position:absolute;top:367.91pt;left:42.1217pt;z-index:63;letter-spacing:-.01em;">silhouette, as stated earlier, a binary image. As an average value,</div></span><img style="position:absolute;left:46.5497pt;top:357.31pt;width:10pt;height:10pt;z-index:37;" src="./target/0222935fc7aa3ca26b5424e70dca272f.png" alt="__rendered_path__37" /><span style="white-space:pre;"><div style="position:absolute;top:379.86pt;left:42.1217pt;z-index:64;letter-spacing:-.006em;">AG is very robust against any errors in individual frames, so we</div></span><span style="white-space:pre;"><div style="position:absolute;top:391.82pt;left:42.1217pt;z-index:65;letter-spacing:.014em;">choose the AG to represent a gait cycle. One sequence yields</div></span><span style="white-space:pre;"><div style="position:absolute;top:403.77pt;left:42.1217pt;z-index:66;letter-spacing:.001em;">several AGs and the number of AGs depends on the number of</div></span><span style="white-space:pre;"><div style="position:absolute;top:415.72pt;left:42.1217pt;z-index:67;letter-spacing:.012em;">gait cycles in the sequence. In the following experiments, the</div></span><span style="white-space:pre;"><div style="position:absolute;top:427.68pt;left:42.1217pt;z-index:68;letter-spacing:-.001em;">AGs provide the data for gait recognition.</div></span><span style="white-space:pre;"><div style="position:absolute;top:439.64pt;left:52.0843pt;z-index:69;letter-spacing:.014em;">The distance deﬁned between the gallery sequence and the</div></span><span style="white-space:pre;"><div style="position:absolute;top:451.59pt;left:42.1217pt;z-index:70;">probe sequence is deﬁned as in [26] by</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:476.84pt;left:44.0654pt;z-index:71;letter-spacing:.006em;">Dist</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:476.84pt;left:66.808pt;z-index:73;letter-spacing:.032em;">AG</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:475.29pt;left:81.819pt;z-index:74;letter-spacing:.011em;">M ethod</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:481.87pt;left:81.819pt;z-index:75;letter-spacing:.006em;">P</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:476.84pt;left:109.13pt;z-index:76;letter-spacing:-.055em;">,</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:476.84pt;left:113.55pt;z-index:77;letter-spacing:.032em;">AG</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:475.29pt;left:128.57pt;z-index:78;letter-spacing:.011em;">M ethod</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:481.87pt;left:128.57pt;z-index:79;letter-spacing:.017em;">G</div></span><img style="position:absolute;left:63.7595pt;top:476.86pt;width:3pt;height:12pt;z-index:72;" src="./target/dffbebac9992b1f105c0a87da100bd6d.png" alt="__rendered_path__72" /><img style="position:absolute;left:156.32pt;top:476.86pt;width:3pt;height:12pt;z-index:80;" src="./target/45445a58b794afd814d1c60c217c62ce.png" alt="__rendered_path__80" /><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:52.3626pt;z-index:81;letter-spacing:-.018em;">=Median</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:494.63pt;left:90.828pt;z-index:82;letter-spacing:.104em;">N</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:502.85pt;left:90.828pt;z-index:84;letter-spacing:.104em;">i</div></span><span class="fmt-20" style="white-space:pre;"><div style="position:absolute;top:497.38pt;left:97.137pt;z-index:83;letter-spacing:-.102em;">p</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:502.85pt;left:93.6454pt;z-index:85;letter-spacing:-.018em;">= 1</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:108.8pt;z-index:87;letter-spacing:-.005em;">min</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:495.59pt;left:125.41pt;z-index:88;letter-spacing:.104em;">N</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:502.85pt;left:125.41pt;z-index:90;letter-spacing:-.081em;">j </div></span><span class="fmt-20" style="white-space:pre;"><div style="position:absolute;top:498.35pt;left:131.71pt;z-index:89;letter-spacing:.017em;">G</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:502.85pt;left:129.11pt;z-index:91;letter-spacing:-.018em;">= 1</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:146.89pt;z-index:93;letter-spacing:.032em;">AG</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:495.96pt;left:161.9pt;z-index:94;letter-spacing:.011em;">M ethod</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:502.53pt;left:161.9pt;z-index:95;letter-spacing:.006em;">P</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:189.21pt;z-index:96;letter-spacing:.014em;">(</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:193.08pt;z-index:97;letter-spacing:.104em;">i</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:196.51pt;z-index:98;letter-spacing:.014em;">)</div></span><span class="fmt-13" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:198.72pt;z-index:99;letter-spacing:-.055em;">−</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:204.81pt;z-index:100;letter-spacing:.032em;">AG</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:495.96pt;left:219.82pt;z-index:101;letter-spacing:.011em;">M ethod</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:502.53pt;left:219.82pt;z-index:102;letter-spacing:.017em;">G</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:247.13pt;z-index:103;letter-spacing:.014em;">(</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:251pt;z-index:104;letter-spacing:-.081em;">j </div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:497.51pt;left:255.67pt;z-index:105;letter-spacing:.014em;">)</div></span><span style="white-space:pre;"><div style="position:absolute;top:497.18pt;left:279.62pt;z-index:107;">(1)</div></span><img style="position:absolute;left:105.75pt;top:497.53pt;width:3pt;height:12pt;z-index:86;" src="./target/2dfa339b977aa1ae14e39448faf833d5.png" alt="__rendered_path__86" /><img style="position:absolute;left:142.8pt;top:496.78pt;width:3pt;height:7pt;z-index:92;" src="./target/86a7789559734b2d43b5821fcf08fee5.png" alt="__rendered_path__92" /><img style="position:absolute;left:261pt;top:496.78pt;width:3pt;height:7pt;z-index:92;" src="./target/86a7789559734b2d43b5821fcf08fee5.png" alt="__rendered_path__92" /><span style="white-space:pre;"><div style="position:absolute;top:522.09pt;left:42.1206pt;z-index:108;">where</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:522.42pt;left:68.2027pt;z-index:109;letter-spacing:.032em;">AG</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:520.87pt;left:83.214pt;z-index:110;letter-spacing:.011em;">M ethod</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:527.45pt;left:83.214pt;z-index:111;letter-spacing:.006em;">P</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:522.42pt;left:110.52pt;z-index:112;letter-spacing:.014em;">(</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:522.42pt;left:114.39pt;z-index:113;letter-spacing:.104em;">i</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:522.42pt;left:117.82pt;z-index:114;letter-spacing:.014em;">)</div></span><span class="fmt-13" style="white-space:pre;"><div style="position:absolute;top:522.42pt;left:121.69pt;z-index:115;letter-spacing:-.222em;">|</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:527.77pt;left:124.45pt;z-index:118;letter-spacing:.104em;">i</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:520.51pt;left:124.45pt;z-index:116;letter-spacing:.104em;">N</div></span><span class="fmt-20" style="white-space:pre;"><div style="position:absolute;top:523.27pt;left:130.76pt;z-index:117;letter-spacing:.006em;">P</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:527.77pt;left:127.27pt;z-index:119;letter-spacing:-.018em;">= 1</div></span><span style="white-space:pre;"><div style="position:absolute;top:522.09pt;left:139.62pt;z-index:120;letter-spacing:-.012em;">is the</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:522.09pt;left:161.93pt;z-index:121;letter-spacing:.001em;">i</div></span><span style="white-space:pre;"><div style="position:absolute;top:522.09pt;left:164.7pt;z-index:122;letter-spacing:-.014em;">th projected AG in the probe data</div></span><img style="position:absolute;left:142.8pt;top:502.75pt;width:3pt;height:7pt;z-index:92;" src="./target/86a7789559734b2d43b5821fcf08fee5.png" alt="__rendered_path__92" /><img style="position:absolute;left:261pt;top:502.75pt;width:3pt;height:7pt;z-index:92;" src="./target/86a7789559734b2d43b5821fcf08fee5.png" alt="__rendered_path__92" /><img style="position:absolute;left:265.53pt;top:497.53pt;width:3pt;height:12pt;z-index:106;" src="./target/7d88b0e7063302e94dc046e29538cf45.png" alt="__rendered_path__106" /><span style="white-space:pre;"><div style="position:absolute;top:534.05pt;left:42.12pt;z-index:123;">and</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:534.38pt;left:60.4084pt;z-index:124;letter-spacing:.032em;">AG</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:532.83pt;left:75.42pt;z-index:125;letter-spacing:.011em;">M ethod</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:539.4pt;left:75.42pt;z-index:126;letter-spacing:.017em;">G</div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:534.38pt;left:102.73pt;z-index:127;letter-spacing:.014em;">(</div></span><span class="fmt-14" style="white-space:pre;"><div style="position:absolute;top:534.38pt;left:106.6pt;z-index:128;letter-spacing:-.081em;">j </div></span><span class="fmt-18" style="white-space:pre;"><div style="position:absolute;top:534.38pt;left:111.27pt;z-index:129;letter-spacing:.014em;">)</div></span><span class="fmt-13" style="white-space:pre;"><div style="position:absolute;top:534.38pt;left:115.14pt;z-index:130;letter-spacing:-.222em;">|</div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:539.72pt;left:117.9pt;z-index:133;letter-spacing:-.081em;">j </div></span><span class="fmt-16" style="white-space:pre;"><div style="position:absolute;top:532.47pt;left:117.9pt;z-index:131;letter-spacing:.104em;">N</div></span><span class="fmt-20" style="white-space:pre;"><div style="position:absolute;top:535.22pt;left:124.21pt;z-index:132;letter-spacing:.017em;">G</div></span><span class="fmt-15" style="white-space:pre;"><div style="position:absolute;top:539.72pt;left:121.6pt;z-index:134;letter-spacing:-.018em;">= 1</div></span><span style="white-space:pre;"><div style="position:absolute;top:534.05pt;left:136.1pt;z-index:135;letter-spacing:-.015em;">is  the</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:534.05pt;left:162.72pt;z-index:136;letter-spacing:.001em;">j</div></span><span style="white-space:pre;"><div style="position:absolute;top:534.05pt;left:165.49pt;z-index:137;letter-spacing:-.018em;">th  projected  AG  in  the  gallery.</div></span><span style="white-space:pre;"><div style="position:absolute;top:546.01pt;left:42.1198pt;z-index:138;letter-spacing:-.014em;">Equation  (1)  uses  the  median  of  the  Euclidean  distances  be-</div></span><span style="white-space:pre;"><div style="position:absolute;top:557.96pt;left:42.1198pt;z-index:139;letter-spacing:.01em;">tween the averaged silhouettes from the probe and the gallery</div></span><span style="white-space:pre;"><div style="position:absolute;top:569.92pt;left:42.1198pt;z-index:140;">sequences.</div></span><span style="white-space:pre;"><div style="position:absolute;top:581.87pt;left:52.0824pt;z-index:141;letter-spacing:.001em;">The difference between (1) and the gait recognition measure</div></span><span style="white-space:pre;"><div style="position:absolute;top:593.82pt;left:42.1198pt;z-index:144;letter-spacing:-.02em;">developed  by  Liu  <span class="fmt-9">et  al</span>.  [26]  is  that  we  choose  a  template  to</div></span><span style="white-space:pre;"><div style="position:absolute;top:605.79pt;left:42.1198pt;z-index:145;letter-spacing:.016em;">pop out only the relevant component for recognition. The al-</div></span><span style="white-space:pre;"><div style="position:absolute;top:617.74pt;left:42.1198pt;z-index:146;letter-spacing:-.016em;">gorithm  is  shown  in  Fig.  5.  We  ﬁrst  segment  each  sequence</div></span><span style="white-space:pre;"><div style="position:absolute;top:629.7pt;left:42.1198pt;z-index:147;letter-spacing:.009em;">in the gallery set (training set) into a few subsequences where</div></span><span style="white-space:pre;"><div style="position:absolute;top:641.65pt;left:42.1198pt;z-index:148;letter-spacing:.013em;">each subsequence is a complete gait cycle. We then calculate</div></span><span style="white-space:pre;"><div style="position:absolute;top:653.6pt;left:42.1198pt;z-index:149;letter-spacing:.018em;">the averaged gait image for each subsequence of images and</div></span><span style="white-space:pre;"><div style="position:absolute;top:665.56pt;left:42.1198pt;z-index:150;letter-spacing:.002em;">use the template to select a component or its complement from</div></span><span style="white-space:pre;"><div style="position:absolute;top:677.52pt;left:42.1198pt;z-index:151;letter-spacing:-.004em;">each averaged gait image. The same procedure is carried out on</div></span><span style="white-space:pre;"><div style="position:absolute;top:689.47pt;left:42.1198pt;z-index:152;letter-spacing:.014em;">the probe set (test set). Finally, the similarities are calculated</div></span><span style="white-space:pre;"><div style="position:absolute;top:701.43pt;left:42.1198pt;z-index:153;letter-spacing:.001em;">between: 1) a testing sample of average gait image (AGI) from</div></span><span style="white-space:pre;"><div style="position:absolute;top:713.38pt;left:42.1198pt;z-index:154;letter-spacing:-.01em;">the probe set (it could be a part or several parts of the entire AG)</div></span><span style="white-space:pre;"><div style="position:absolute;top:725.33pt;left:42.1198pt;z-index:155;letter-spacing:-.005em;">and 2) all AGIs stored in the gallery set. Then, this sample from</div></span><span style="white-space:pre;"><div style="position:absolute;top:737.29pt;left:42.1198pt;z-index:156;letter-spacing:.001em;">the probe set is recognized by (1).</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:315.46pt;z-index:14;letter-spacing:.001em;">H<span class="fmt-11">UMAN</span> G<span class="fmt-11">AIT</span> R<span class="fmt-11">ECOGNITION</span></div></span><img style="position:absolute;left:64.854pt;top:91.656pt;width:468.6pt;height:176.28pt;z-index:15;" src="./target/80d6b54eee77e6fdeae75aa0768ce6e9.png" alt="Image_103_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:531.6pt;left:305.14pt;z-index:158;">Fig. 5.</div><div style="position:absolute;top:531.6pt;left:334.69pt;z-index:158;">Extraction of the average gait and the similarity measure.</div></span><img style="position:absolute;left:307.67pt;top:282.88pt;width:246pt;height:237.6pt;z-index:157;" src="./target/8c22100dfb6986730ded41024a682bf6.png" alt="Image_9_0" /><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:562.94pt;left:305.14pt;z-index:159;letter-spacing:.003em;">B.  Experimental Results</div></span><span style="white-space:pre;"><div style="position:absolute;top:579.88pt;left:315.1pt;z-index:160;letter-spacing:-.011em;">We  ﬁrst  conduct  15  groups  of experiments  to examine  the</div></span><span style="white-space:pre;"><div style="position:absolute;top:591.83pt;left:305.14pt;z-index:161;letter-spacing:-.007em;">effectiveness of the averaged gait image, each component of the</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.79pt;left:305.14pt;z-index:162;letter-spacing:.013em;">averaged gait image, and the complement of each component</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.75pt;left:305.14pt;z-index:163;letter-spacing:-.009em;">for gait recognition. Table II lists the recognition rates. Note that</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.7pt;left:305.14pt;z-index:164;letter-spacing:-.006em;">different from the data set provided in Table I, we here focus on</div></span><span style="white-space:pre;"><div style="position:absolute;top:639.65pt;left:305.14pt;z-index:165;letter-spacing:.001em;">the contribution of different parts/component of human AGI to</div></span><span style="white-space:pre;"><div style="position:absolute;top:651.61pt;left:305.14pt;z-index:166;">human recognition.</div></span><span style="white-space:pre;"><div style="position:absolute;top:663.56pt;left:315.1pt;z-index:167;letter-spacing:-.016em;">We  also  conduct  a  number  of  experiments  to  examine  the</div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:305.14pt;z-index:168;letter-spacing:.004em;">cooccurrence effects of pairs of components on the gait recog-</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:305.14pt;z-index:169;letter-spacing:.01em;">nition. It would be difﬁcult to conduct experiments on all 252</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:305.14pt;z-index:170;letter-spacing:-.002em;">pairs and get useful information from them. Therefore, we only</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:305.14pt;z-index:171;letter-spacing:-.001em;">conduct experiments on pairs of two parts, which may possibly</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:305.14pt;z-index:172;letter-spacing:-.01em;">reduce  the  recognition  rates  or  may  not  impact  the  recogni-</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:305.14pt;z-index:173;letter-spacing:.019em;">tion rate compared to the performance obtained by using the</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 5</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-6"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:60.1753pt;z-index:3;letter-spacing:-.002em;">: GAIT COMPONENTS AND THEIR APPLICATION TO GENDER RECOGNITION</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:65.7237pt;left:277.5pt;z-index:5;letter-spacing:-.01em;">TABLE III</div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:76.2068pt;left:272.08pt;z-index:11;letter-spacing:-.003em;">ESULTS FOR</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:37.908pt;z-index:2;">LI <span class="fmt-10">et al.</span></div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:188.42pt;z-index:10;letter-spacing:-.003em;">R<span class="fmt-11">ANK</span> <span>1 E</span><span class="fmt-11">XPERIMENTAL</span> <span>R</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:354.23pt;left:37.908pt;z-index:19;letter-spacing:.002em;">whole averaged gait image (i.e., we only select pairs for which</div></span><span style="white-space:pre;"><div style="position:absolute;top:366.19pt;left:37.908pt;z-index:20;letter-spacing:.002em;">strong effects are expected). For example, in probe A, both the</div></span><span style="white-space:pre;"><div style="position:absolute;top:378.15pt;left:37.908pt;z-index:21;letter-spacing:-.007em;">ﬁfth component and the seventh component have a negative im-</div></span><span style="white-space:pre;"><div style="position:absolute;top:390.1pt;left:37.908pt;z-index:22;letter-spacing:.007em;">pact on the recognition rate, while the complement of the ﬁfth</div></span><span style="white-space:pre;"><div style="position:absolute;top:402.05pt;left:37.908pt;z-index:23;letter-spacing:.004em;">component and the complement of the seventh component can</div></span><span style="white-space:pre;"><div style="position:absolute;top:414.01pt;left:37.908pt;z-index:24;">improve the recognition rate. Therefore, we need to investigate</div></span><span style="white-space:pre;"><div style="position:absolute;top:425.96pt;left:37.908pt;z-index:25;letter-spacing:.015em;">the performance when we remove both the components from</div></span><span style="white-space:pre;"><div style="position:absolute;top:437.92pt;left:37.908pt;z-index:26;letter-spacing:-.016em;">the  averaged  gait  image.  All  results  from  the  examination  of</div></span><span style="white-space:pre;"><div style="position:absolute;top:449.88pt;left:37.908pt;z-index:27;letter-spacing:-.002em;">co-occurrence effects are shown in Table III.</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:476.77pt;left:37.908pt;z-index:28;letter-spacing:.004em;">C.  Discussion</div></span><span style="white-space:pre;"><div style="position:absolute;top:493.71pt;left:47.8706pt;z-index:29;letter-spacing:-.003em;">We ﬁrst evaluate the effect of each component on gait recog-</div></span><span style="white-space:pre;"><div style="position:absolute;top:505.66pt;left:37.908pt;z-index:30;letter-spacing:-.012em;">nition,  as  shown  in Table  II. We  deem  that  a component  has</div></span><span style="white-space:pre;"><div style="position:absolute;top:517.61pt;left:37.908pt;z-index:31;letter-spacing:-.017em;">a  positive  effect  on  gait  recognition  if  the  recognition  rate  is</div></span><span style="white-space:pre;"><div style="position:absolute;top:529.57pt;left:37.908pt;z-index:32;letter-spacing:-.001em;">signiﬁcantly reduced when the component is removed from the</div></span><span style="white-space:pre;"><div style="position:absolute;top:541.53pt;left:37.908pt;z-index:33;letter-spacing:-.017em;">averaged  gait  image.  A  component  has  a  negative  impact  on</div></span><span style="white-space:pre;"><div style="position:absolute;top:553.48pt;left:37.908pt;z-index:34;letter-spacing:-.001em;">gait recognition if the recognition rate is signiﬁcantly increased</div></span><span style="white-space:pre;"><div style="position:absolute;top:565.93pt;left:37.908pt;z-index:35;">when the component is removed from the averaged gait image.</div></span><span style="white-space:pre;"><div style="position:absolute;top:578.39pt;left:37.908pt;z-index:36;letter-spacing:.004em;">Otherwise, we deem that the component has little effect on the</div></span><span style="white-space:pre;"><div style="position:absolute;top:590.84pt;left:37.908pt;z-index:37;letter-spacing:-.008em;">gait recognition. For example, the head has a positive impact for</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.29pt;left:37.908pt;z-index:38;letter-spacing:-.008em;">the probe A test, because the recognition rate is 77 if we remove</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.75pt;left:37.908pt;z-index:39;letter-spacing:.003em;">the head from the averaged gait image and the recognition rate</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.7pt;left:37.908pt;z-index:40;letter-spacing:-.007em;">is 83 if we use all components for recognition. The front-leg has</div></span><span style="white-space:pre;"><div style="position:absolute;top:639.65pt;left:37.908pt;z-index:41;letter-spacing:.001em;">a negative contribution for the probe A test, because the recog-</div></span><span style="white-space:pre;"><div style="position:absolute;top:651.62pt;left:37.908pt;z-index:42;letter-spacing:.007em;">nition rate is 84 without the front-leg, and the recognition rate</div></span><span style="white-space:pre;"><div style="position:absolute;top:663.57pt;left:37.908pt;z-index:43;letter-spacing:.01em;">is 83 with all components for recognition. The trunk has little</div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:37.908pt;z-index:44;letter-spacing:-.007em;">effect on the recognition rate in probe B because the recognition</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:37.908pt;z-index:45;letter-spacing:-.001em;">rate is the same with or without the trunk. A full analysis of the</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:37.908pt;z-index:46;letter-spacing:-.004em;">results is given in Table IV.</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:47.8706pt;z-index:47;letter-spacing:.014em;">The preliminary analysis in Table IV and the experimental</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.35pt;left:37.908pt;z-index:48;letter-spacing:-.017em;">results  in  Tables  II  and  III  are  summarized  by  the  following</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:37.908pt;z-index:49;letter-spacing:-.002em;">observations listed in Table V for all probes.</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:541.54pt;z-index:4;letter-spacing:-.001em;">149</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:311.25pt;z-index:17;letter-spacing:.001em;">H<span class="fmt-11">UMAN</span> G<span class="fmt-11">AIT</span> R<span class="fmt-11">ECOGNITION</span></div></span><img style="position:absolute;left:45.621pt;top:91.6758pt;width:498.61pt;height:250.12pt;z-index:18;" src="./target/5a7727844fc3628ee7f8e1b7d9a79242.png" alt="Image_46_0" /><span style="white-space:pre;"><div style="position:absolute;top:354.24pt;left:310.89pt;z-index:50;letter-spacing:-.002em;">The following conclusions are drawn from Table V. For per-</div></span><span style="white-space:pre;"><div style="position:absolute;top:366.19pt;left:300.92pt;z-index:51;letter-spacing:.009em;">son identiﬁcation, head, arm, trunk, and back-leg usually pro-</div></span><span style="white-space:pre;"><div style="position:absolute;top:378.15pt;left:300.92pt;z-index:52;letter-spacing:.011em;">vide the best discriminating information. The inclusion of the</div></span><span style="white-space:pre;"><div style="position:absolute;top:390.11pt;left:300.92pt;z-index:53;letter-spacing:.006em;">front-leg usually reduces the recognition rate. When the probe</div></span><span style="white-space:pre;"><div style="position:absolute;top:402.06pt;left:300.92pt;z-index:54;letter-spacing:.015em;">set differs from the gallery set in terms of walking surface or</div></span><span style="white-space:pre;"><div style="position:absolute;top:414.02pt;left:300.92pt;z-index:55;letter-spacing:-.007em;">the carrying of a briefcase, the inclusion of the thigh reduces the</div></span><span style="white-space:pre;"><div style="position:absolute;top:425.97pt;left:300.92pt;z-index:56;letter-spacing:.011em;">recognition rate. Otherwise, the inclusion of the thigh usually</div></span><span style="white-space:pre;"><div style="position:absolute;top:437.92pt;left:300.92pt;z-index:57;letter-spacing:-.013em;">increases  the  recognition  rate.  The  usefulness  of  the  feet  for</div></span><span style="white-space:pre;"><div style="position:absolute;top:449.88pt;left:300.92pt;z-index:58;letter-spacing:-.002em;">recognition is unclear.</div></span><span style="white-space:pre;"><div style="position:absolute;top:479.26pt;left:372.69pt;z-index:61;letter-spacing:-.01em;">V.  E <span class="fmt-7">XPERIMENTS ON</span> <span>HGR</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:496.19pt;left:310.89pt;z-index:62;letter-spacing:-.003em;">Gender recognition has received a fair amount of attention in</div></span><span style="white-space:pre;"><div style="position:absolute;top:508.15pt;left:300.92pt;z-index:63;letter-spacing:-.01em;">the  psychophysical  and  computer  vision  literature,  especially</div></span><span style="white-space:pre;"><div style="position:absolute;top:520.1pt;left:300.92pt;z-index:64;letter-spacing:.011em;">in the case of gender recognition based on the face. There are</div></span><span style="white-space:pre;"><div style="position:absolute;top:532.06pt;left:300.92pt;z-index:65;letter-spacing:.009em;">relatively few gait-based studies. In [17], a human face image</div></span><span style="white-space:pre;"><div style="position:absolute;top:544.02pt;left:300.92pt;z-index:66;letter-spacing:-.003em;">is  treated  as  a  vector,  and  independent  component  analysis</div></span><span style="white-space:pre;"><div style="position:absolute;top:555.97pt;left:300.92pt;z-index:67;letter-spacing:-.004em;">(ICA) is then applied to reduce the dimension of the data space.</div></span><span style="white-space:pre;"><div style="position:absolute;top:567.92pt;left:300.92pt;z-index:68;letter-spacing:-.016em;">A  support  vector  machine  (SVM)  is  used  to  further  improve</div></span><span style="white-space:pre;"><div style="position:absolute;top:579.88pt;left:300.92pt;z-index:69;letter-spacing:.016em;">the classiﬁcation performance. The SVM is also used in [28]</div></span><span style="white-space:pre;"><div style="position:absolute;top:591.84pt;left:300.92pt;z-index:70;letter-spacing:.008em;">for gender recognition based on face images. In [36], an input</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.79pt;left:300.92pt;z-index:71;letter-spacing:.005em;">image is coarsely divided into face, hair, and clothing regions,</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.75pt;left:300.92pt;z-index:72;letter-spacing:-.001em;">and a model is learned independently for each region. The ﬁnal</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.7pt;left:300.92pt;z-index:73;letter-spacing:.021em;">classiﬁcation of the face image is made by using a Bayesian</div></span><span style="white-space:pre;"><div style="position:absolute;top:639.65pt;left:300.92pt;z-index:74;letter-spacing:.002em;">approach. Local regions are used for gender recognition in [3]:</div></span><span style="white-space:pre;"><div style="position:absolute;top:651.61pt;left:300.92pt;z-index:79;letter-spacing:-.009em;">the similarity values between <span class="fmt-9">N</span> local regions and <span class="fmt-9">M</span> face images</div></span><span style="white-space:pre;"><div style="position:absolute;top:663.57pt;left:300.92pt;z-index:80;letter-spacing:.013em;">in the training set are used as features, PCA is used to reduce</div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:300.92pt;z-index:81;letter-spacing:.014em;">the dimension of the feature space and remove the noise, and</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:300.92pt;z-index:82;letter-spacing:.016em;">then an SVM or Fisher linear discriminant analysis (LDA) is</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:300.92pt;z-index:83;letter-spacing:-.013em;">used  for  the  ﬁnal  classiﬁcation.  The  algorithm  achieves  high</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:300.92pt;z-index:84;letter-spacing:.003em;">recognition  accuracy  on  a  database  with  13 000  frontal  or</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:300.92pt;z-index:85;letter-spacing:-.009em;">nearly  frontal  face  images.  In  the  future,  locality  preserving</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:300.92pt;z-index:86;letter-spacing:-.016em;">projections  (LPP)  [15]  could  be  the  basis  of  an  algorithm  to</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 6</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-7"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:42.12pt;z-index:1;">150</div><div style="position:absolute;top:34.3462pt;left:128.36pt;z-index:1;letter-spacing:-.003em;">IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART C: APPLICATIONS AND REVIEWS, VOL. 38, NO. 2, MARCH 2008</div></span><span style="white-space:pre;"><div style="position:absolute;top:65.7237pt;left:281.48pt;z-index:2;letter-spacing:-.023em;">TABLE IV</div></span><span style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:142.63pt;z-index:18;letter-spacing:-.003em;">P<span class="fmt-11">RELIMINARY</span> <span>E</span><span class="fmt-11">VALUATION OF THE</span> <span>I</span><span class="fmt-11">MPACT OF</span> <span>E</span><span class="fmt-11">ACH</span> <span>C</span><span class="fmt-11">OMPONENT ON</span> <span>H</span><span class="fmt-11">UMAN</span> <span>G</span><span class="fmt-11">AIT</span> <span>R</span><span class="fmt-11">ECOGNITION</span></div></span><img style="position:absolute;left:130.62pt;top:91.6996pt;width:337pt;height:206.35pt;z-index:19;" src="./target/b1acda627ada8eb2df59a77957152623.png" alt="Image_81_0" /><span style="white-space:pre;"><div style="position:absolute;top:310.46pt;left:282.82pt;z-index:20;letter-spacing:-.026em;">TABLE V</div></span><span style="white-space:pre;"><div style="position:absolute;top:319.52pt;left:241.56pt;z-index:26;letter-spacing:-.01em;">O<span class="fmt-11">BSERVATIONS</span> <span>F</span><span class="fmt-11">ROM</span> <span>E</span><span class="fmt-11">XPERIMENTS</span></div></span><img style="position:absolute;left:58.392pt;top:336.41pt;width:481.49pt;height:321.61pt;z-index:27;" src="./target/13043837e6f855248be29880fa6025cb.png" alt="Image_125_0" /><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:675.44pt;left:42.12pt;z-index:34;letter-spacing:-.003em;">select  features  for  subsequent  gender  classiﬁcation,  e.g.,  by</div><div style="position:absolute;top:675.45pt;left:305.13pt;z-index:34;letter-spacing:.017em;">that the averaged gait images for males and females differ in</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:687.4pt;left:42.12pt;z-index:35;letter-spacing:-.006em;">an  SVM.  LPP  has  been  proposed  to  discover  the  nonlinear</div><div style="position:absolute;top:687.4pt;left:305.13pt;z-index:35;letter-spacing:.018em;">several ways, for example: 1) for the arm (with breast) com-</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:699.36pt;left:42.12pt;z-index:36;letter-spacing:.009em;">structure of data, which lie on or nearly on a low-dimensional</div><div style="position:absolute;top:699.36pt;left:305.13pt;z-index:36;letter-spacing:-.003em;">ponent,  the  breast  area  of  females  is  not  so  ﬂat  as  that  of</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:711.31pt;left:42.12pt;z-index:37;letter-spacing:.001em;">manifold embedded in a high-dimensional space.</div><div style="position:absolute;top:711.31pt;left:305.13pt;z-index:37;letter-spacing:.008em;">males; 2) for the trunk component, the back neck area is more</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:723.26pt;left:52.0826pt;z-index:38;letter-spacing:.015em;">In this paper, we show that the averaged gait image can be</div><div style="position:absolute;top:723.26pt;left:305.13pt;z-index:38;letter-spacing:-.001em;">curved for males than for females, possibly because males tend</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:735.22pt;left:42.12pt;z-index:39;letter-spacing:.015em;">used for gender recognition. An examination of Fig. 6 shows</div><div style="position:absolute;top:735.22pt;left:305.13pt;z-index:39;letter-spacing:.02em;">to have less hair than females; and 3) difference on all other</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 7</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-6"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:60.1753pt;z-index:3;letter-spacing:-.002em;">: GAIT COMPONENTS AND THEIR APPLICATION TO GENDER RECOGNITION</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:65.7237pt;left:277.27pt;z-index:5;letter-spacing:-.023em;">TABLE VI</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:297.18pt;z-index:14;letter-spacing:.001em;">R</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:37.908pt;z-index:2;">LI <span class="fmt-10">et al.</span></div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:145.22pt;z-index:13;letter-spacing:-.002em;">E<span class="fmt-11">XPERIMENTAL</span> <span>R</span><span class="fmt-11">ESULTS FOR</span> <span>H</span><span class="fmt-11">UMAN</span> <span>G</span><span class="fmt-11">ENDER</span></div></span><img style="position:absolute;left:92.754pt;top:282.92pt;width:141.3pt;height:86.6039pt;z-index:25;" src="./target/0c8696f728af4e0510387eb615d284d1.png" alt="Image_68_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:380.43pt;left:37.908pt;z-index:26;letter-spacing:.004em;">Fig. 6.</div><div style="position:absolute;top:380.43pt;left:67.6763pt;z-index:26;letter-spacing:.005em;">Male versus female. The left subﬁgure is the mean image of all male</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:389.4pt;left:37.908pt;z-index:27;letter-spacing:.01em;">averaged gait images, and the right subﬁgure is the mean image of all female</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:398.36pt;left:37.908pt;z-index:28;letter-spacing:-.001em;">averaged gait images.</div></span><span style="white-space:pre;"><div style="position:absolute;top:421.78pt;left:37.908pt;z-index:29;letter-spacing:.009em;">gait parts/components, e.g., the front-leg component, the dark</div></span><span style="white-space:pre;"><div style="position:absolute;top:433.74pt;left:37.908pt;z-index:30;">area of the lower part is wider for females. The performance of</div></span><span style="white-space:pre;"><div style="position:absolute;top:445.69pt;left:37.908pt;z-index:31;letter-spacing:-.004em;">the averaged gait-image-based gender recognition is impressive</div></span><span style="white-space:pre;"><div style="position:absolute;top:457.64pt;left:37.908pt;z-index:32;letter-spacing:.014em;">compared with the normal human ability to recognize gender</div></span><span style="white-space:pre;"><div style="position:absolute;top:469.61pt;left:37.908pt;z-index:33;">using motion information.</div></span><span style="white-space:pre;"><div style="position:absolute;top:481.56pt;left:47.8706pt;z-index:34;letter-spacing:-.012em;">In the following, the averaged gait-image-based gender recog-</div></span><span style="white-space:pre;"><div style="position:absolute;top:493.51pt;left:37.908pt;z-index:35;letter-spacing:.003em;">nition algorithm is described brieﬂy. Then, we conduct experi-</div></span><span style="white-space:pre;"><div style="position:absolute;top:505.47pt;left:37.908pt;z-index:36;letter-spacing:.003em;">ments to demonstrate the impact of each component on gender</div></span><span style="white-space:pre;"><div style="position:absolute;top:517.42pt;left:37.908pt;z-index:37;letter-spacing:.01em;">recognition. Finally, an analysis of the experimental results is</div></span><span style="white-space:pre;"><div style="position:absolute;top:529.38pt;left:37.908pt;z-index:38;letter-spacing:-.006em;">given.</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:555.63pt;left:37.908pt;z-index:39;letter-spacing:.001em;">A.  AGI-Based Gender Recognition</div></span><span style="white-space:pre;"><div style="position:absolute;top:572.57pt;left:47.8706pt;z-index:40;letter-spacing:.008em;">The gender recognition procedure is shown in Fig. 7. In the</div></span><span style="white-space:pre;"><div style="position:absolute;top:584.52pt;left:37.908pt;z-index:41;letter-spacing:-.012em;">gallery,  we  ﬁrst  segment  each  sequence  into  a  subsequences</div></span><span style="white-space:pre;"><div style="position:absolute;top:596.47pt;left:37.908pt;z-index:42;letter-spacing:.001em;">such that each subsequence corresponds to a complete walking</div></span><span style="white-space:pre;"><div style="position:absolute;top:608.43pt;left:37.908pt;z-index:43;letter-spacing:.008em;">period. We then calculate the averaged gait image and use the</div></span><span style="white-space:pre;"><div style="position:absolute;top:620.38pt;left:37.908pt;z-index:44;letter-spacing:-.011em;">template to select a part or its complement in all subsequences in</div></span><span style="white-space:pre;"><div style="position:absolute;top:632.33pt;left:37.908pt;z-index:45;letter-spacing:-.014em;">the gallery and probe sets. We train a linear SVM classiﬁer based</div></span><span style="white-space:pre;"><div style="position:absolute;top:644.3pt;left:37.908pt;z-index:46;letter-spacing:.004em;">on the averaged gait image, a selected part, or its complement.</div></span><span style="white-space:pre;"><div style="position:absolute;top:656.25pt;left:37.908pt;z-index:47;letter-spacing:.006em;">The averaged gait image, a selected part, or its complement of</div></span><span style="white-space:pre;"><div style="position:absolute;top:668.2pt;left:37.908pt;z-index:48;letter-spacing:.002em;">a sample in a probe is classiﬁed by the trained SVM. Finally, a</div></span><span style="white-space:pre;"><div style="position:absolute;top:680.16pt;left:37.908pt;z-index:49;">voting scheme is used to obtain the ﬁnal decision.</div></span><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:706.4pt;left:37.908pt;z-index:50;letter-spacing:-.001em;">B.  Recognition Performance</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:47.8706pt;z-index:51;letter-spacing:-.015em;">We  ﬁrst  conduct  15  groups  of experiments  to  examine  the</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:37.908pt;z-index:52;letter-spacing:-.01em;">performances of the averaged gait image, each component of the</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:541.54pt;z-index:4;letter-spacing:-.001em;">151</div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:76.2068pt;left:302.5pt;z-index:23;letter-spacing:-.007em;">ECOGNITION <span class="fmt-7">F</span>ROM <span class="fmt-7">A</span>VERAGED <span class="fmt-7">G</span>AIT <span class="fmt-7">I</span>MAGES</div></span><img style="position:absolute;left:60.705pt;top:91.6861pt;width:468.4pt;height:176.25pt;z-index:24;" src="./target/56277967fd8bdabe049953d911afd92c.png" alt="Image_24_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:472.11pt;left:300.92pt;z-index:54;letter-spacing:.004em;">Fig. 7.</div><div style="position:absolute;top:472.11pt;left:330.68pt;z-index:54;letter-spacing:.004em;">SVM-based gender recognition algorithm. The ﬁve images at the top</div></span><img style="position:absolute;left:347.61pt;top:282.96pt;width:157.6pt;height:178.23pt;z-index:53;" src="./target/239b4db41a5b23e80eae7509a29674b7.png" alt="Image_111_0" /><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:481.07pt;left:300.92pt;z-index:55;letter-spacing:.003em;">of the ﬁgure are the AGIs of a sequence, which represents a person’s gait. The</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:490.05pt;left:300.92pt;z-index:56;letter-spacing:.017em;">block in the middle of the ﬁgure with some averaged gait images represents</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:499.01pt;left:300.92pt;z-index:57;letter-spacing:.008em;">the training set and the SVM classiﬁer. The blocks at the bottom of the ﬁgure</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:507.97pt;left:300.92pt;z-index:58;letter-spacing:.005em;">represent the voting scheme. The test sample represented by the ﬁve averaged</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:516.94pt;left:300.92pt;z-index:59;letter-spacing:.001em;">gait images is classiﬁed as female.</div></span><span style="white-space:pre;"><div style="position:absolute;top:555.97pt;left:300.92pt;z-index:60;letter-spacing:.013em;">averaged gait image, and the complement of each component</div></span><span style="white-space:pre;"><div style="position:absolute;top:567.92pt;left:300.92pt;z-index:61;letter-spacing:-.001em;">for gender recognition. Table VI shows the recognition rates.</div></span><span style="white-space:pre;"><div style="position:absolute;top:579.88pt;left:310.89pt;z-index:62;letter-spacing:-.016em;">We  also  conduct  a  number  of  experiments  to  examine  the</div></span><span style="white-space:pre;"><div style="position:absolute;top:591.83pt;left:300.92pt;z-index:63;letter-spacing:.005em;">cooccurrence effects of pairs of parts on gender recognition. It</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.79pt;left:300.92pt;z-index:64;letter-spacing:.007em;">would be difﬁcult to conduct experiments on all 252 pairs and</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.75pt;left:300.92pt;z-index:65;letter-spacing:.006em;">get useful information from them. Therefore, we only conduct</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.7pt;left:300.92pt;z-index:66;letter-spacing:-.002em;">experiments on pairs of parts, which can reduce the recognition</div></span><span style="white-space:pre;"><div style="position:absolute;top:639.65pt;left:300.92pt;z-index:67;letter-spacing:-.016em;">rate or have little effect on the recognition rate. Then, whether the</div></span><span style="white-space:pre;"><div style="position:absolute;top:651.61pt;left:300.92pt;z-index:68;letter-spacing:.004em;">omission of the two parts causes a reduction in the recognition</div></span><span style="white-space:pre;"><div style="position:absolute;top:663.57pt;left:300.92pt;z-index:69;letter-spacing:.012em;">rate will be examined later. For example, in probe A, the ﬁrst</div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:300.92pt;z-index:70;letter-spacing:.005em;">part, the fourth part, the ﬁfth part, and the sixth part have little</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:300.92pt;z-index:71;letter-spacing:-.007em;">effect on the recognition rate, because the complements of these</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:300.92pt;z-index:72;letter-spacing:.002em;">components do not reduce or improve the recognition rate. We</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:300.92pt;z-index:73;letter-spacing:-.003em;">investigate the recognition rates when both of the selected parts</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:300.92pt;z-index:74;">are removed from the averaged gait image. All the results from</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:300.92pt;z-index:75;letter-spacing:-.002em;">these experiments are shown in Table VII.</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 8</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-7"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:42.12pt;z-index:1;">152</div><div style="position:absolute;top:34.3462pt;left:128.36pt;z-index:1;letter-spacing:-.003em;">IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART C: APPLICATIONS AND REVIEWS, VOL. 38, NO. 2, MARCH 2008</div></span><span style="white-space:pre;"><div style="position:absolute;top:65.7237pt;left:280.16pt;z-index:2;letter-spacing:-.021em;">TABLE VII</div></span><span style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:163.49pt;z-index:16;letter-spacing:-.003em;">E<span class="fmt-11">XPERIMENTAL</span> <span>R</span><span class="fmt-11">ESULTS FOR</span> <span>G</span><span class="fmt-11">ENDER</span> <span>R</span><span class="fmt-11">ECOGNITION FROM</span> <span>A</span><span class="fmt-11">VERAGED</span> <span>G</span><span class="fmt-11">AIT</span> <span>I</span><span class="fmt-11">MAGES</span></div></span><img style="position:absolute;left:48.357pt;top:91.647pt;width:501.6pt;height:315.96pt;z-index:17;" src="./target/69cf9395e1102ec13d190117a792eb7b.png" alt="Image_10_0" /><span style="white-space:pre;"><div style="position:absolute;top:420.02pt;left:278.83pt;z-index:18;letter-spacing:-.009em;">TABLE VIII</div></span><span style="white-space:pre;"><div style="position:absolute;top:429.09pt;left:150.94pt;z-index:32;letter-spacing:-.003em;">P<span class="fmt-11">RELIMINARY</span> <span>E</span><span class="fmt-11">VALUATION OF THE</span> <span>I</span><span class="fmt-11">MPACT OF</span> <span>E</span><span class="fmt-11">ACH</span> <span>C</span><span class="fmt-11">OMPONENT ON</span> <span>G</span><span class="fmt-11">ENDER</span> <span>R</span><span class="fmt-11">ECOGNITION</span></div></span><img style="position:absolute;left:131.16pt;top:445.95pt;width:336pt;height:208.56pt;z-index:33;" src="./target/ba1c7703acb53820dabac3e59bae2464.png" alt="Image_54_0" /><span class="fmt-9" style="white-space:pre;"><div style="position:absolute;top:670.44pt;left:42.12pt;z-index:40;letter-spacing:.004em;">C.  Discussion</div><div style="position:absolute;top:670.44pt;left:305.13pt;z-index:40;" class="fmt-6">gender recognition if the recognition rate is increased when the</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:687.38pt;left:52.0826pt;z-index:41;letter-spacing:.003em;">The following observations and conclusions are drawn from</div><div style="position:absolute;top:682.39pt;left:305.13pt;z-index:41;letter-spacing:-.015em;">component  is  removed  from  the  averaged  gait  image.  Other-</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:699.33pt;left:42.12pt;z-index:42;letter-spacing:-.022em;">Tables  VI  and  VII.  In  the  case  of  Table  VI,  we  deem  that  a</div><div style="position:absolute;top:694.35pt;left:305.13pt;z-index:42;letter-spacing:.017em;">wise, we deem that the component has little effect on gender</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:711.29pt;left:42.12pt;z-index:43;letter-spacing:-.015em;">component  has  a  positive  effect  on  gender  recognition  if  the</div><div style="position:absolute;top:706.31pt;left:305.13pt;z-index:43;letter-spacing:.008em;">recognition. For example, the arm has a positive effect for the</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:723.24pt;left:42.12pt;z-index:44;letter-spacing:-.01em;">recognition rate decreases when the component is removed from</div><div style="position:absolute;top:718.26pt;left:305.13pt;z-index:44;letter-spacing:.023em;">probe A test, because the recognition rate is 96 if the arm is</div></span><span class="fmt-6" style="white-space:pre;"><div style="position:absolute;top:735.2pt;left:42.12pt;z-index:45;letter-spacing:.002em;">the averaged gait image. A component has a negative effect on</div><div style="position:absolute;top:730.22pt;left:305.13pt;z-index:45;letter-spacing:.001em;">removed from the averaged gait image and the recognition rate</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 9</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-6"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:60.1753pt;z-index:3;letter-spacing:-.002em;">: GAIT COMPONENTS AND THEIR APPLICATION TO GENDER RECOGNITION</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:65.7237pt;left:277.27pt;z-index:5;letter-spacing:-.023em;">TABLE IX</div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:76.2068pt;left:291.37pt;z-index:9;letter-spacing:-.025em;">ROM</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:37.908pt;z-index:2;">LI <span class="fmt-10">et al.</span></div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:237.35pt;z-index:8;letter-spacing:-.018em;">O<span class="fmt-11">BSERVATIONS</span> <span>F</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:309.99pt;left:37.908pt;z-index:13;letter-spacing:.011em;">is 98 if all components are used for the recognition. The head</div></span><span style="white-space:pre;"><div style="position:absolute;top:321.94pt;left:37.908pt;z-index:14;letter-spacing:.018em;">has a negative effect for the probe B test, because the recog-</div></span><span style="white-space:pre;"><div style="position:absolute;top:333.89pt;left:37.908pt;z-index:15;letter-spacing:.015em;">nition rate is 100 without the head and the recognition rate is</div></span><span style="white-space:pre;"><div style="position:absolute;top:345.86pt;left:37.908pt;z-index:16;letter-spacing:.001em;">98 if all components are included. The thigh has little effect on</div></span><span style="white-space:pre;"><div style="position:absolute;top:357.81pt;left:37.908pt;z-index:17;letter-spacing:.003em;">the recognition rate for probe A because the recognition rate is</div></span><span style="white-space:pre;"><div style="position:absolute;top:369.76pt;left:37.908pt;z-index:18;">same with or without trunk.</div></span><span style="white-space:pre;"><div style="position:absolute;top:381.72pt;left:47.8706pt;z-index:19;letter-spacing:.008em;">Based on the preliminary analysis in Table VIII and the ex-</div></span><span style="white-space:pre;"><div style="position:absolute;top:393.67pt;left:37.908pt;z-index:20;letter-spacing:-.003em;">perimental results in Tables VI and VII, we make the following</div></span><span style="white-space:pre;"><div style="position:absolute;top:405.63pt;left:37.908pt;z-index:21;letter-spacing:-.002em;">observations listed in Table IX for all probes.</div></span><span style="white-space:pre;"><div style="position:absolute;top:417.59pt;left:47.8706pt;z-index:22;letter-spacing:.007em;">The following conclusions are drawn from Table IX. Trunk</div></span><span style="white-space:pre;"><div style="position:absolute;top:429.54pt;left:37.908pt;z-index:23;letter-spacing:-.008em;">and front-leg usually increase the gender recognition rate. When</div></span><span style="white-space:pre;"><div style="position:absolute;top:441.49pt;left:37.908pt;z-index:24;letter-spacing:-.008em;">the probe set is different from the gallery set in terms of surface,</div></span><span style="white-space:pre;"><div style="position:absolute;top:453.45pt;left:37.908pt;z-index:25;letter-spacing:-.011em;">arm  reduces  the  recognition  rate,  and  when  the  probe  set  is</div></span><span style="white-space:pre;"><div style="position:absolute;top:465.4pt;left:37.908pt;z-index:26;letter-spacing:-.008em;">different from the gallery set in terms of briefcase, thigh reduces</div></span><span style="white-space:pre;"><div style="position:absolute;top:477.36pt;left:37.908pt;z-index:27;letter-spacing:.002em;">the recognition rate. Otherwise, arm and thigh usually increase</div></span><span style="white-space:pre;"><div style="position:absolute;top:489.32pt;left:37.908pt;z-index:28;letter-spacing:-.011em;">the  recognition  rate.  The  head,  back-leg,  and  feet  have  only</div></span><span style="white-space:pre;"><div style="position:absolute;top:501.27pt;left:37.908pt;z-index:29;">negligible effects on the gender recognition rate.</div></span><span style="white-space:pre;"><div style="position:absolute;top:527.08pt;left:127.85pt;z-index:31;letter-spacing:.003em;">VI.  C<span class="fmt-7">ONCLUSION</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:544.02pt;left:47.871pt;z-index:32;letter-spacing:-.003em;">With  strong  empirical  evaluation,  this  paper  ﬁrst  focuses</div></span><span style="white-space:pre;"><div style="position:absolute;top:555.97pt;left:37.9084pt;z-index:33;letter-spacing:.016em;">on the idea of using silhouette-based gait analysis for gender</div></span><span style="white-space:pre;"><div style="position:absolute;top:567.92pt;left:37.9084pt;z-index:34;letter-spacing:-.007em;">recognition—in  details,  the  application  of  human  gait  infor-</div></span><span style="white-space:pre;"><div style="position:absolute;top:579.88pt;left:37.9084pt;z-index:35;letter-spacing:.008em;">mation to human recognition and gender recognition has been</div></span><span style="white-space:pre;"><div style="position:absolute;top:591.83pt;left:37.9084pt;z-index:36;">studied. The following has been demonstrated.</div></span><span style="white-space:pre;"><div style="position:absolute;top:603.79pt;left:47.0431pt;z-index:37;letter-spacing:-.015em;">1)  The gait of the head, arm, trunk, and back-leg are important</div></span><span style="white-space:pre;"><div style="position:absolute;top:615.75pt;left:60.8224pt;z-index:38;">for averaged gait-based human recognition.</div></span><span style="white-space:pre;"><div style="position:absolute;top:627.7pt;left:47.0431pt;z-index:39;letter-spacing:.007em;">2)  The inclusion of the gait of the front-leg usually reduces</div></span><span style="white-space:pre;"><div style="position:absolute;top:639.65pt;left:60.8224pt;z-index:40;letter-spacing:.001em;">the recognition rate.</div></span><span style="white-space:pre;"><div style="position:absolute;top:651.61pt;left:47.0431pt;z-index:41;">3)  The contributions of the feet to recognition are not clear.</div></span><span style="white-space:pre;"><div style="position:absolute;top:663.57pt;left:47.0431pt;z-index:42;">4)  The inclusion of the gait of the thigh reduces the recogni-</div></span><span style="white-space:pre;"><div style="position:absolute;top:675.53pt;left:60.8224pt;z-index:43;letter-spacing:-.013em;">tion rate when the probe set is different from the gallery set</div></span><span style="white-space:pre;"><div style="position:absolute;top:687.48pt;left:60.8224pt;z-index:44;letter-spacing:-.001em;">in terms of walking surface or the carrying of a briefcase.</div></span><span style="white-space:pre;"><div style="position:absolute;top:699.43pt;left:47.0431pt;z-index:45;letter-spacing:.003em;">5)  The gaits of the trunk and front-leg are usually important</div></span><span style="white-space:pre;"><div style="position:absolute;top:711.39pt;left:60.8224pt;z-index:46;">for gender recognition.</div></span><span style="white-space:pre;"><div style="position:absolute;top:723.34pt;left:47.0431pt;z-index:47;letter-spacing:.014em;">6)  The gaits of the head, back-leg, and feet are not helpful</div></span><span style="white-space:pre;"><div style="position:absolute;top:735.3pt;left:60.8224pt;z-index:48;">for gender recognition.</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:541.54pt;z-index:4;letter-spacing:-.001em;">153</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:74.7865pt;left:307.64pt;z-index:11;">E<span class="fmt-11">XPERIMENTS</span></div></span><img style="position:absolute;left:64.215pt;top:91.6821pt;width:461.39pt;height:205.86pt;z-index:12;" src="./target/d2c15fd17ba38b7f5b94e304a78605d7.png" alt="Image_90_0" /><span style="white-space:pre;"><div style="position:absolute;top:309.99pt;left:310.06pt;z-index:49;">7)  The gaits of the arm and thigh reduce the gender recogni-</div></span><span style="white-space:pre;"><div style="position:absolute;top:321.95pt;left:323.84pt;z-index:50;letter-spacing:-.013em;">tion rate when the probe set is different from the gallery set</div></span><span style="white-space:pre;"><div style="position:absolute;top:333.9pt;left:323.84pt;z-index:51;letter-spacing:-.001em;">in terms of walking surface or the carrying of a briefcase.</div></span><span style="white-space:pre;"><div style="position:absolute;top:345.86pt;left:300.92pt;z-index:52;letter-spacing:.01em;">Gait analysis is difﬁcult because of the wide variety of move-</div></span><span style="white-space:pre;"><div style="position:absolute;top:357.81pt;left:300.92pt;z-index:53;letter-spacing:-.006em;">ments of the different parts of the body, but at the same time the</div></span><span style="white-space:pre;"><div style="position:absolute;top:369.76pt;left:300.92pt;z-index:54;letter-spacing:.007em;">experiments suggest that a large amount of useful information</div></span><span style="white-space:pre;"><div style="position:absolute;top:381.72pt;left:300.92pt;z-index:55;letter-spacing:.013em;">can be obtained from gait, e.g., according to our experiences,</div></span><span style="white-space:pre;"><div style="position:absolute;top:393.68pt;left:300.92pt;z-index:56;letter-spacing:.002em;">if one falls down on the street, the change of his feet silhouette</div></span><span style="white-space:pre;"><div style="position:absolute;top:405.63pt;left:300.92pt;z-index:57;letter-spacing:-.012em;">component has a large probability to lead a change of other com-</div></span><span style="white-space:pre;"><div style="position:absolute;top:417.59pt;left:300.92pt;z-index:58;letter-spacing:-.015em;">ponents. The AGI has been shown to be effective for both human</div></span><span style="white-space:pre;"><div style="position:absolute;top:429.54pt;left:300.92pt;z-index:59;letter-spacing:.008em;">ID recognition and gender recognition. Nevertheless, the AGI</div></span><span style="white-space:pre;"><div style="position:absolute;top:441.49pt;left:300.92pt;z-index:60;letter-spacing:-.01em;">ignores the temporal information that is intuitively useful for un-</div></span><span style="white-space:pre;"><div style="position:absolute;top:453.46pt;left:300.92pt;z-index:61;letter-spacing:-.002em;">derstanding a gait. Our future work will focus on the robust ex-</div></span><span style="white-space:pre;"><div style="position:absolute;top:465.41pt;left:300.92pt;z-index:62;letter-spacing:-.011em;">traction and modeling of the dynamic properties of a walking se-</div></span><span style="white-space:pre;"><div style="position:absolute;top:477.37pt;left:300.92pt;z-index:63;letter-spacing:-.007em;">quence, and then, use these properties for dynamic gait analysis.</div></span><span style="white-space:pre;"><div style="position:absolute;top:498.24pt;left:386.03pt;z-index:65;letter-spacing:-.002em;">A<span class="fmt-7">CKNOWLEDGMENT</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:515.17pt;left:310.89pt;z-index:66;letter-spacing:.003em;">The authors would like to thank the Associate Editor and all</div></span><span style="white-space:pre;"><div style="position:absolute;top:527.12pt;left:300.92pt;z-index:67;letter-spacing:-.002em;">reviewers for their constructive comments.</div></span><span style="white-space:pre;"><div style="position:absolute;top:548pt;left:400.76pt;z-index:69;letter-spacing:.001em;">R<span class="fmt-7">EFERENCES</span></div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:566.71pt;left:304.91pt;z-index:71;letter-spacing:.01em;">[1]  J. Aggarwal and Q. Cai, “Human motion analysis: A review,”  <span class="fmt-21">Comput.</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:575.68pt;left:319.18pt;z-index:73;letter-spacing:-.002em;">Vis. Image Understanding<span class="fmt-7">, vol. 73, pp. 428–440, 1999.</span></div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:584.65pt;left:304.91pt;z-index:74;letter-spacing:.002em;">[2]  C. Ben-Abdelkader, R. Cutler, and L. Davis, “Person identiﬁcation using</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:593.61pt;left:319.18pt;z-index:77;letter-spacing:-.01em;">automatic height and stride estimation,” in <span class="fmt-21">Proc. Int. Conf. Pattern Recog.</span>,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:602.57pt;left:319.18pt;z-index:78;letter-spacing:-.001em;">2002, vol. 4, pp. 377–380.</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:611.55pt;left:304.91pt;z-index:79;letter-spacing:-.005em;">[3]  C. Ben-Abdelkader and P. Grifﬁn, “A local region-based approach to gen-</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:620.51pt;left:319.18pt;z-index:81;">der classiﬁcation from face images,” in <span class="fmt-21">Proc. IEEE Comput. Vis. Pattern</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:629.47pt;left:319.18pt;z-index:83;letter-spacing:-.001em;">Recog.<span class="fmt-7">, Jun. 2005, vol. 3, pp. 52–52.</span></div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:638.44pt;left:304.91pt;z-index:84;letter-spacing:.018em;">[4]  A. Bissacco, A. Chiuso, Y. Ma, and S. Soatto, “Recognition of human</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:647.41pt;left:319.18pt;z-index:87;letter-spacing:-.012em;">gaits,” in <span class="fmt-21">Proc. IEEE Comput. Vis. Pattern Recog.</span>, Kauai, HI, 2001, vol. 2,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:656.37pt;left:319.18pt;z-index:88;">pp. 52–57.</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:665.34pt;left:304.91pt;z-index:89;letter-spacing:-.001em;">[5]  J. E. Boyd and J. J. Little, “Motion from transient oscillations,” presented</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:674.3pt;left:319.18pt;z-index:90;">at the IEEE Int. Conf. Pattern Recog., Tech. Sketeches CD-ROM. Kauai,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:683.28pt;left:319.18pt;z-index:91;">HI, 2001.</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:692.24pt;left:304.91pt;z-index:92;letter-spacing:.001em;">[6]  A.  Bobick  and  A.  Johnson,  “Gait  recognition  using  static  activity—</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:701.2pt;left:319.18pt;z-index:95;letter-spacing:.013em;">Speciﬁc parameters,” in <span class="fmt-21">IEEE Comput. Vis. Pattern Recog.</span>, Kauai, HI,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:710.17pt;left:319.18pt;z-index:96;letter-spacing:-.001em;">2001, vol. 1, pp. 423–430.</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:719.14pt;left:304.91pt;z-index:97;letter-spacing:.016em;">[7]  J. E. Boyd, “Synchronization of oscillations for machine perception of</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:728.1pt;left:319.18pt;z-index:100;letter-spacing:-.014em;">gaits,”   <span class="fmt-21">Comput.  Vis.  Image  Understanding</span>,  vol.  96,  no.  1,  pp.  35–59,</div></span><span class="fmt-7" style="white-space:pre;"><div style="position:absolute;top:737.07pt;left:319.18pt;z-index:101;">2004.</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 10</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-7"><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:42.12pt;z-index:1;">154</div><div style="position:absolute;top:34.3462pt;left:128.36pt;z-index:1;letter-spacing:-.003em;">IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART C: APPLICATIONS AND REVIEWS, VOL. 38, NO. 2, MARCH 2008</div></span><span style="white-space:pre;"><div style="position:absolute;top:67.5326pt;left:46.107pt;z-index:2;letter-spacing:-.004em;">[8]  R. T. Collins, R. Bross, and J. Shi, “Silhouette-based human identiﬁcation</div></span><span style="white-space:pre;"><div style="position:absolute;top:76.4965pt;left:60.3806pt;z-index:5;letter-spacing:-.009em;">from body shape and gait,” in <span class="fmt-21">Proc. IEEE Face Gesture</span>, Washington, DC,</div></span><span style="white-space:pre;"><div style="position:absolute;top:85.4693pt;left:60.3806pt;z-index:6;">2002, pp. 351–356.</div></span><span style="white-space:pre;"><div style="position:absolute;top:94.4332pt;left:46.107pt;z-index:7;letter-spacing:-.01em;">[9]  D. Cunado, M. Nixon, and J. Carter, “Automatic extraction and description</div></span><span style="white-space:pre;"><div style="position:absolute;top:103.4pt;left:60.3806pt;z-index:9;letter-spacing:-.012em;">of  human  gait  models  for  recognition  purposes,”   <span class="fmt-21">Comput.  Vis.  Image</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:112.36pt;left:60.3806pt;z-index:11;letter-spacing:-.001em;">Understanding<span class="fmt-7">, vol. 90, no. 1, pp. 1–41, 2003.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:121.33pt;left:42.1195pt;z-index:12;letter-spacing:.012em;">[10]  R. Cutler and L. Davis, “Robust periodic motion and motion symmetry</div></span><span style="white-space:pre;"><div style="position:absolute;top:130.3pt;left:60.3806pt;z-index:15;letter-spacing:-.015em;">detection,”  in  <span class="fmt-21">Proc.  IEEE  Int.  Conf.  Pattern  Recog.</span>,  Hilton  Head,  SC,</div></span><span style="white-space:pre;"><div style="position:absolute;top:139.26pt;left:60.3806pt;z-index:16;">2000, pp. 615–622.</div></span><span style="white-space:pre;"><div style="position:absolute;top:148.23pt;left:42.1195pt;z-index:17;">[11]  J. Cutting and D. Profﬁtt, “Gait perception as an example of how we may</div></span><span style="white-space:pre;"><div style="position:absolute;top:157.2pt;left:60.3806pt;z-index:20;letter-spacing:.003em;">perceive events,”   <span class="fmt-21">Intersens. Percept. Sens. Integr.</span>, vol. 2, pp. 249–273,</div></span><span style="white-space:pre;"><div style="position:absolute;top:166.16pt;left:60.3814pt;z-index:21;">1981.</div></span><span style="white-space:pre;"><div style="position:absolute;top:175.13pt;left:42.1203pt;z-index:22;letter-spacing:-.014em;">[12]  J.  W.  Davis  and  A.  F.  Bobick,  “The  representation  and  recognition  of</div></span><span style="white-space:pre;"><div style="position:absolute;top:184.09pt;left:60.3814pt;z-index:24;letter-spacing:-.004em;">human movement using temporal templates,” in <span class="fmt-21">Proc. IEEE Comput. Vis.</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:193.06pt;left:60.3814pt;z-index:26;letter-spacing:-.002em;">Pattern Recog.<span class="fmt-7">, San Juan, Puerto Rico, 1997, pp. 928–934.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:202.03pt;left:42.1203pt;z-index:28;letter-spacing:-.007em;">[13]  D. Gavrila, “The visual analysis of human movement: A survey,” <span class="fmt-21">Comput.</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:210.99pt;left:60.3806pt;z-index:30;letter-spacing:-.002em;">Vis. Image Understanding<span class="fmt-7">, vol. 73, pp. 82–98, 1999.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:219.96pt;left:42.1195pt;z-index:31;letter-spacing:-.01em;">[14]  J.  Han  and  B.  Bhanu,  “Statistical  feature  fusion  for  gait-based  human</div></span><span style="white-space:pre;"><div style="position:absolute;top:228.93pt;left:60.3806pt;z-index:34;letter-spacing:.012em;">recognition,” in <span class="fmt-21">Proc. IEEE Comput. Vis. Pattern Recog.</span>, 2004, vol. 2,</div></span><span style="white-space:pre;"><div style="position:absolute;top:237.89pt;left:60.3806pt;z-index:35;">pp. 842–847.</div></span><span style="white-space:pre;"><div style="position:absolute;top:246.86pt;left:42.1195pt;z-index:36;letter-spacing:-.007em;">[15]  X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang, “Face recognition using</div></span><span style="white-space:pre;"><div style="position:absolute;top:255.83pt;left:60.3806pt;z-index:39;letter-spacing:.004em;">laplacianfaces,”  <span class="fmt-21">IEEE Trans. Pattern Anal. Mach. Intell.</span>, vol. 27, no. 3,</div></span><span style="white-space:pre;"><div style="position:absolute;top:264.79pt;left:60.3814pt;z-index:40;letter-spacing:-.002em;">pp. 328–3410, Mar. 2005.</div></span><span style="white-space:pre;"><div style="position:absolute;top:273.76pt;left:42.1203pt;z-index:41;letter-spacing:-.013em;">[16]  W. Hu, T. Tan, L. Wang, and S. Maybank, “A survey on visual surveillance</div></span><span style="white-space:pre;"><div style="position:absolute;top:282.72pt;left:60.3814pt;z-index:44;letter-spacing:.001em;">of object  motion and behaviors,”   <span class="fmt-21">IEEE  Trans.  Syst.,  Man Cybern. C.</span>,</div></span><span style="white-space:pre;"><div style="position:absolute;top:291.69pt;left:60.3814pt;z-index:45;letter-spacing:-.001em;">vol. 34, no. 3, pp. 334–352, Aug. 2004.</div></span><span style="white-space:pre;"><div style="position:absolute;top:300.66pt;left:42.1203pt;z-index:46;letter-spacing:.007em;">[17]  A. Jian and J. Huang, “Integrating independent components and support</div></span><span style="white-space:pre;"><div style="position:absolute;top:309.62pt;left:60.3814pt;z-index:48;letter-spacing:-.011em;">vector  machines  for  gender  classiﬁcation,”  in  <span class="fmt-21">Proc.  Int.  Conf.  Pattern</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:318.59pt;left:60.3814pt;z-index:50;letter-spacing:-.001em;">Recog.<span class="fmt-7">, 2004, vol. 3, pp. 558–561.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:327.56pt;left:42.1203pt;z-index:53;letter-spacing:-.009em;">[18]  G. Johansson, “Visual motion perception,” <span class="fmt-21">Sci. Amer.</span>, vol. 232, pp. 76–88,</div></span><span style="white-space:pre;"><div style="position:absolute;top:336.52pt;left:60.3814pt;z-index:54;">1975.</div></span><span style="white-space:pre;"><div style="position:absolute;top:345.49pt;left:42.1203pt;z-index:55;letter-spacing:.013em;">[19]  A. Kale, A. Sundaresan, A. N. Rajagopalan, N. P. Cuntoor, A. K. Roy-</div></span><span style="white-space:pre;"><div style="position:absolute;top:354.45pt;left:60.3814pt;z-index:56;letter-spacing:.005em;">Chowdhury, V. Kruger, and R. Chellappa, “Identiﬁcation of humans us-</div></span><span style="white-space:pre;"><div style="position:absolute;top:363.42pt;left:60.3814pt;z-index:59;letter-spacing:.011em;">ing gait,”   <span class="fmt-21">IEEE Trans. Image Process.</span>, vol. 13, no. 9, pp. 1163–1173,</div></span><span style="white-space:pre;"><div style="position:absolute;top:372.39pt;left:60.3814pt;z-index:60;">2004.</div></span><span style="white-space:pre;"><div style="position:absolute;top:381.35pt;left:42.1203pt;z-index:63;letter-spacing:.017em;">[20]  S. Kung, M. Mak, and S. Lin, <span class="fmt-21">Biometric Authentication</span>.</div><div style="position:absolute;top:381.35pt;left:257.16pt;z-index:63;letter-spacing:-.004em;">Englewood</div></span><span style="white-space:pre;"><div style="position:absolute;top:390.32pt;left:60.3814pt;z-index:64;">Cliffs, NJ: Prentice-Hall, 2004.</div></span><span style="white-space:pre;"><div style="position:absolute;top:399.29pt;left:42.1203pt;z-index:65;letter-spacing:-.006em;">[21]  L. Lee, G. Dalley, and K. Tieu, “Learning pedestrian models for silhouette</div></span><span style="white-space:pre;"><div style="position:absolute;top:408.25pt;left:60.3814pt;z-index:68;letter-spacing:.007em;">reﬁnement,” in <span class="fmt-21">Proc. IEEE Int. Conf. Comput. Vis.</span>, Nice, France, 2003,</div></span><span style="white-space:pre;"><div style="position:absolute;top:417.22pt;left:60.3814pt;z-index:69;letter-spacing:-.001em;">vol. 1, pp. 663–670.</div></span><span style="white-space:pre;"><div style="position:absolute;top:426.19pt;left:42.1203pt;z-index:70;letter-spacing:-.008em;">[22]  L. Lee and W. E. L. Grimson, “Gait analysis for recognition and classiﬁca-</div></span><span style="white-space:pre;"><div style="position:absolute;top:435.15pt;left:60.3814pt;z-index:73;letter-spacing:-.003em;">tion,” in <span class="fmt-21">Proc. IEEE Face Gesture</span>, Washington, DC, 2002, pp. 155–162.</div></span><span style="white-space:pre;"><div style="position:absolute;top:444.12pt;left:42.1211pt;z-index:74;letter-spacing:.021em;">[23]  J. Little and J. Boyd, “Recognizing people by their gait: The shape of</div></span><span style="white-space:pre;"><div style="position:absolute;top:453.08pt;left:60.3822pt;z-index:77;letter-spacing:-.003em;">motion,”  <span class="fmt-21">Videre</span>, vol. 1, no. 2, pp. 1–32, 1998.</div></span><span style="white-space:pre;"><div style="position:absolute;top:462.05pt;left:42.1211pt;z-index:79;letter-spacing:-.003em;">[24]  F. Liu and R. W. Picard, “Finding periodicity in space and time,” in <span class="fmt-21">Proc.</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:471.02pt;left:60.3822pt;z-index:81;letter-spacing:-.002em;">IEEE Int. Conf. Comput. Vis.<span class="fmt-7">, 1998, pp. 376–383.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:479.98pt;left:42.1211pt;z-index:82;letter-spacing:.008em;">[25]  Z. Liu, L. Malave, and S. Sarkar, “Studies on silhouette quality and gait</div></span><span style="white-space:pre;"><div style="position:absolute;top:488.94pt;left:60.3822pt;z-index:85;letter-spacing:.012em;">recognition,” in <span class="fmt-21">Proc. IEEE Comput. Vis. Pattern Recog.</span>, 2004, vol. 2,</div></span><span style="white-space:pre;"><div style="position:absolute;top:497.92pt;left:60.3822pt;z-index:86;">pp. 704–711.</div></span><span style="white-space:pre;"><div style="position:absolute;top:506.88pt;left:42.1211pt;z-index:87;letter-spacing:.015em;">[26]  Z. Liu and S. Sarkar, “Simplest representation yet for gait recognition:</div></span><span style="white-space:pre;"><div style="position:absolute;top:515.85pt;left:60.3822pt;z-index:90;letter-spacing:-.012em;">Averaged silhouette,” in <span class="fmt-21">Proc. Int. Conf. Pattern Recog.</span>, Cambridge, U.K.,</div></span><span style="white-space:pre;"><div style="position:absolute;top:524.82pt;left:60.383pt;z-index:91;letter-spacing:-.001em;">2004, vol. 4, pp. 211–214.</div></span><span style="white-space:pre;"><div style="position:absolute;top:533.78pt;left:42.1219pt;z-index:92;letter-spacing:-.001em;">[27]  T. Moeslund and E. Granum, “A survey of computer vision-based human</div></span><span style="white-space:pre;"><div style="position:absolute;top:542.75pt;left:60.383pt;z-index:95;letter-spacing:-.009em;">motion  capture,”    <span class="fmt-21">Comput.  Vis.  Image  Understanding</span>,  vol.  81,  no.  3,</div></span><span style="white-space:pre;"><div style="position:absolute;top:551.71pt;left:60.383pt;z-index:96;">pp. 231–268, 2001.</div></span><span style="white-space:pre;"><div style="position:absolute;top:560.68pt;left:42.1219pt;z-index:97;letter-spacing:.007em;">[28]  B. Moghaddam and M.-H. Yang, “Learning gender with support faces,”</div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:569.65pt;left:60.383pt;z-index:99;letter-spacing:-.006em;">IEEE Trans. Pattern Anal. Mach. Intell.<span class="fmt-7">, vol. 24, no. 5, pp. 707–711, May</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:578.61pt;left:60.383pt;z-index:100;">2002.</div></span><span style="white-space:pre;"><div style="position:absolute;top:587.57pt;left:42.1219pt;z-index:101;letter-spacing:.003em;">[29]  M. Murray, A. Drought, and R. Kory, “Walking pattern of normal men,”</div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:596.55pt;left:60.383pt;z-index:103;letter-spacing:-.002em;">J. Bone Joint Surg.<span class="fmt-7">, vol. 46-A, no. 2, pp. 335–360, 1964.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:605.51pt;left:42.1219pt;z-index:104;letter-spacing:.013em;">[30]  S. Niyogi and E. Adelson, “Analyzing and recognizing walking ﬁgures</div></span><span style="white-space:pre;"><div style="position:absolute;top:614.48pt;left:60.383pt;z-index:107;letter-spacing:-.004em;">in XYT,” in <span class="fmt-21">Proc. IEEE Comput. Vis. Pattern Recog.</span>, Seattle, WA, 1994,</div></span><span style="white-space:pre;"><div style="position:absolute;top:623.44pt;left:60.3822pt;z-index:108;">pp. 469–474.</div></span><span style="white-space:pre;"><div style="position:absolute;top:632.41pt;left:42.1211pt;z-index:109;letter-spacing:.011em;">[31]  P. Phillips, S. Sarkar, I. Robledo, P. Grother, and K. Bowyer, “The gait</div></span><span style="white-space:pre;"><div style="position:absolute;top:641.38pt;left:60.3822pt;z-index:110;letter-spacing:-.006em;">identiﬁcation  challenge  problem:  Datasets  and  baseline  algorithm,”  in</div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:650.34pt;left:60.3822pt;z-index:112;letter-spacing:-.003em;">Proc. IEEE Int. Conf. Pattern Recog.<span class="fmt-7">, 2002, vol. 1, pp. 385–388.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:659.3pt;left:42.1219pt;z-index:113;letter-spacing:.011em;">[32]  S. Sarkar, P. Phillips, Z. Liu, I. Vega, P. Grother, and K. Bowyer, “The</div></span><span style="white-space:pre;"><div style="position:absolute;top:668.28pt;left:60.383pt;z-index:114;letter-spacing:.007em;">humanid gait challenge problem: Data sets, performance, and analysis,”</div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:677.24pt;left:60.383pt;z-index:116;letter-spacing:-.004em;">IEEE Trans. Pattern Anal. Mach. Intell.<span class="fmt-7">, vol. 27, no. 2, pp. 162–177, Feb.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:686.2pt;left:60.383pt;z-index:117;">2005.</div></span><span style="white-space:pre;"><div style="position:absolute;top:695.18pt;left:42.1219pt;z-index:118;letter-spacing:-.012em;">[33]  R.  Tanawongsuwan  and  A.  Bobick,  “Modelling  the  effects  of  walking</div></span><span style="white-space:pre;"><div style="position:absolute;top:704.14pt;left:60.383pt;z-index:120;letter-spacing:-.003em;">speed on appearance-based gait recognition,” in <span class="fmt-21">Proc. IEEE Comput. Vis.</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:713.11pt;left:60.383pt;z-index:122;letter-spacing:-.003em;">Pattern Recog.<span class="fmt-7">, Washington, DC, 2004, vol. 2, pp. 783–790.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:722.07pt;left:42.1219pt;z-index:123;letter-spacing:-.016em;">[34]  D.  Tao,  X.  Li,  X.  Wu,  and  S.  Maybank,  “Elapsed  time  in  human  gait</div></span><span style="white-space:pre;"><div style="position:absolute;top:731.04pt;left:60.383pt;z-index:126;letter-spacing:.016em;">recognition: A new approach,” in <span class="fmt-21">Proc. IEEE ICASSP</span>, 2006, pp. 177–</div></span><span style="white-space:pre;"><div style="position:absolute;top:740.01pt;left:60.383pt;z-index:127;">180.</div></span><span style="white-space:pre;"><div style="position:absolute;top:67.527pt;left:305.14pt;z-index:128;letter-spacing:.001em;">[35]  D. Tao, X. Li, X. Wu, and S. Maybank, “Human carrying status in visual</div></span><span style="white-space:pre;"><div style="position:absolute;top:76.491pt;left:323.4pt;z-index:131;letter-spacing:-.012em;">surveillance,” in <span class="fmt-21">Proc. IEEE Comput. Vis. Pattern Recog.</span>, 2006, pp. 1670–</div></span><span style="white-space:pre;"><div style="position:absolute;top:85.4637pt;left:323.4pt;z-index:132;">11677.</div></span><span style="white-space:pre;"><div style="position:absolute;top:94.4277pt;left:305.14pt;z-index:133;letter-spacing:.008em;">[36]  K. Ueki, H. Komatsu, S. Imaizumi, K. Kaneko, N. Sekine, J. Katto, and</div></span><span style="white-space:pre;"><div style="position:absolute;top:103.39pt;left:323.4pt;z-index:134;letter-spacing:.012em;">T. Kobayashi, “A method of gender classiﬁcation by integrating facial,</div></span><span style="white-space:pre;"><div style="position:absolute;top:112.36pt;left:323.4pt;z-index:137;">hairstyle, and clothing images,” in <span class="fmt-21">Proc. Int. Conf. Pattern Recog.</span>, 2004,</div></span><span style="white-space:pre;"><div style="position:absolute;top:121.33pt;left:323.4pt;z-index:138;">pp. 446–449.</div></span><span style="white-space:pre;"><div style="position:absolute;top:130.29pt;left:305.14pt;z-index:139;letter-spacing:-.007em;">[37]  G. Veres, L. Gordon, J. Carter, and M. Nixon, “What image information is</div></span><span style="white-space:pre;"><div style="position:absolute;top:139.26pt;left:323.4pt;z-index:141;letter-spacing:.005em;">important in silhouette-based gait recognition?,” in <span class="fmt-21">Proc. IEEE Comput.</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:148.22pt;left:323.4pt;z-index:143;letter-spacing:-.004em;">Vis. Pattern Recog.<span class="fmt-7">, Washington, DC, 2004, vol. 2, pp. 776–782.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:157.19pt;left:305.14pt;z-index:144;letter-spacing:-.008em;">[38]  L. Wang, H. Ning, T. Tan, and W. Hu, “Fusion of static and dynamic body</div></span><span style="white-space:pre;"><div style="position:absolute;top:166.16pt;left:323.4pt;z-index:147;letter-spacing:.01em;">biometrics for gait recognition,” in <span class="fmt-21">Proc. IEEE Int. Conf. Comput. Vis.</span>,</div></span><span style="white-space:pre;"><div style="position:absolute;top:175.12pt;left:323.4pt;z-index:148;letter-spacing:-.001em;">2003, vol. 2, p. 1449.</div></span><span style="white-space:pre;"><div style="position:absolute;top:184.09pt;left:305.14pt;z-index:149;letter-spacing:-.017em;">[39]  L.  Wang,  T.  Tan,  H.  Ning,  and  W.  Hu,  “Silhouette  analysis-based  gait</div></span><span style="white-space:pre;"><div style="position:absolute;top:193.06pt;left:323.4pt;z-index:151;letter-spacing:.002em;">recognition for human identiﬁcation,”  <span class="fmt-21">IEEE Trans. Pattern Anal. Mach.</span></div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:202.02pt;left:323.4pt;z-index:153;">Intell.<span class="fmt-7">, vol. 25, no. 12, pp. 1505–1518, Dec. 2003.</span></div></span><span class="fmt-22" style="white-space:pre;"><div style="position:absolute;top:222.95pt;left:388.82pt;z-index:156;letter-spacing:.008em;">Xuelong Li <span class="fmt-7">(M’02–SM’07) holds a permanent post</span></div></span><img style="position:absolute;left:305.14pt;top:224.61pt;width:71.7pt;height:89.625pt;z-index:154;" src="./target/9625286b8c17fd1e0e490bdf8870f717.png" alt="Image_126_0" /><span style="white-space:pre;"><div style="position:absolute;top:231.92pt;left:388.82pt;z-index:157;letter-spacing:.007em;">in the School of Computer Science and Information</div></span><span style="white-space:pre;"><div style="position:absolute;top:240.88pt;left:388.82pt;z-index:158;letter-spacing:-.01em;">Systems,  Birkbeck  College,  University  of  London,</div></span><span style="white-space:pre;"><div style="position:absolute;top:249.85pt;left:388.82pt;z-index:159;letter-spacing:-.01em;">London, U.K. and is a Visiting Professor with Tianjin</div></span><span style="white-space:pre;"><div style="position:absolute;top:258.82pt;left:388.82pt;z-index:160;letter-spacing:.002em;">University, Tianjin, China. His research interests in-</div></span><span style="white-space:pre;"><div style="position:absolute;top:267.78pt;left:388.82pt;z-index:161;letter-spacing:-.004em;">clude  digital  image/video  processing,  analysis,  re-</div></span><span style="white-space:pre;"><div style="position:absolute;top:276.75pt;left:388.82pt;z-index:162;letter-spacing:-.005em;">trieval, and indexing, pattern recognition, biometrics,</div></span><span style="white-space:pre;"><div style="position:absolute;top:285.71pt;left:388.82pt;z-index:163;letter-spacing:-.007em;">and  visual  surveillance.  His  research  activities  are</div></span><span style="white-space:pre;"><div style="position:absolute;top:294.68pt;left:388.82pt;z-index:164;letter-spacing:-.013em;">partly sponsored by the Engineering and Physical Sci-</div></span><span style="white-space:pre;"><div style="position:absolute;top:303.65pt;left:388.82pt;z-index:165;letter-spacing:.009em;">ences Research Council, the British Council, Royal</div></span><span style="white-space:pre;"><div style="position:absolute;top:312.61pt;left:388.82pt;z-index:166;letter-spacing:-.008em;">Society, etc. He has around 90 scientiﬁc publications.</div></span><span style="white-space:pre;"><div style="position:absolute;top:321.58pt;left:314.67pt;z-index:173;letter-spacing:-.009em;">Dr. Li is an Associate Editor of the IEEE T<span class="fmt-11">RANSACTIONS ON</span> <span>S</span><span class="fmt-11">YSTEMS</span><span>, M</span><span class="fmt-11">AN</span><span>,</span></div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:331.97pt;left:305.14pt;z-index:185;letter-spacing:-.009em;">AND  <span class="fmt-7">C</span>YBERNETICS<span class="fmt-7">—</span>PART  <span class="fmt-7">B:  C</span>YBERNETICS<span class="fmt-7">,  IEEE  T</span>RANSACTIONS  ON  <span class="fmt-7">S</span>YS<span class="fmt-7">-</span></div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:340.93pt;left:305.14pt;z-index:199;letter-spacing:-.001em;">TEMS<span class="fmt-7">, M</span>AN<span class="fmt-7">,</span> AND <span class="fmt-7">C</span>YBERNETICS<span class="fmt-7">—</span>PART <span class="fmt-7">C: A</span>PPLICATIONS AND <span class="fmt-7">R</span>EVIEWS<span class="fmt-7">, IEEE </span></div></span><span style="white-space:pre;"><div style="position:absolute;top:348.48pt;left:305.14pt;z-index:209;letter-spacing:-.007em;">T<span class="fmt-11">RANSACTIONS ON</span> <span>I</span><span class="fmt-11">MAGE</span> <span>P</span><span class="fmt-11">ROCESSING</span><span>, and IEEE T</span><span class="fmt-11">RANSACTIONS ON</span> <span>C</span><span class="fmt-11">IRCUITS</span></div></span><span class="fmt-11" style="white-space:pre;"><div style="position:absolute;top:358.87pt;left:305.14pt;z-index:217;letter-spacing:.019em;">AND <span class="fmt-7">S</span><span>YSTEMS  FOR</span> <span class="fmt-7">V</span><span>IDEO</span> <span class="fmt-7">T</span><span>ECHNOLOGY</span><span class="fmt-7">. He is also an Editor of two books,</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:366.41pt;left:305.14pt;z-index:218;letter-spacing:.007em;">an Editorial Board Member of several other journals, and a Guest Coeditor of</div></span><span style="white-space:pre;"><div style="position:absolute;top:375.38pt;left:305.14pt;z-index:219;letter-spacing:-.002em;">seven special issues. He is the recipient of several best paper awards and nomi-</div></span><span style="white-space:pre;"><div style="position:absolute;top:384.34pt;left:305.14pt;z-index:220;letter-spacing:-.008em;">nations. He has served as a Chair or a Cochair of a dozen conferences, including</div></span><span style="white-space:pre;"><div style="position:absolute;top:393.31pt;left:305.14pt;z-index:221;letter-spacing:.003em;">as a Publicity Cochair of the 2008 IEEE International Conference on Systems,</div></span><span style="white-space:pre;"><div style="position:absolute;top:402.28pt;left:305.14pt;z-index:222;letter-spacing:-.015em;">Man,  and  Cybernetics  (SMC),  and  a  Program  Committee  Member  for  more</div></span><span style="white-space:pre;"><div style="position:absolute;top:411.24pt;left:305.14pt;z-index:223;letter-spacing:-.005em;">than 70 conferences, including the annual SMC conferences. He is a member of</div></span><span style="white-space:pre;"><div style="position:absolute;top:420.21pt;left:305.14pt;z-index:224;letter-spacing:.003em;">several technical committees, including the IEEE SMC Technical Committees</div></span><span style="white-space:pre;"><div style="position:absolute;top:429.18pt;left:305.14pt;z-index:225;letter-spacing:.006em;">on Computational Intelligence, Machine Learning, and Self-Organization and</div></span><span style="white-space:pre;"><div style="position:absolute;top:438.14pt;left:305.14pt;z-index:226;">Cybernetics for Informatics.</div></span><span class="fmt-22" style="white-space:pre;"><div style="position:absolute;top:462.06pt;left:388.82pt;z-index:229;letter-spacing:-.012em;">Stephen  J.  Maybank  <span class="fmt-7">(M’97–SM’06)  received  the</span></div></span><img style="position:absolute;left:305.14pt;top:463.71pt;width:71.7pt;height:89.625pt;z-index:227;" src="./target/90aeeb76f6fcb0d9e5f815a22a6d4ce8.png" alt="Image_32_0" /><span style="white-space:pre;"><div style="position:absolute;top:471.02pt;left:388.82pt;z-index:230;letter-spacing:-.007em;">B.A.  degree  in  mathematics  from  King’s  College,</div></span><span style="white-space:pre;"><div style="position:absolute;top:479.98pt;left:388.82pt;z-index:231;letter-spacing:.015em;">Cambridge, U.K., in 1976, and the Ph.D. degree in</div></span><span style="white-space:pre;"><div style="position:absolute;top:488.95pt;left:388.82pt;z-index:232;letter-spacing:-.003em;">computer science from Birkbeck College, University</div></span><span style="white-space:pre;"><div style="position:absolute;top:497.92pt;left:388.82pt;z-index:233;">of London, London, U.K., in 1988.</div></span><span style="white-space:pre;"><div style="position:absolute;top:506.89pt;left:400.02pt;z-index:234;letter-spacing:-.015em;">From  1980  to  1995,  he  was  a  Research  Scien-</div></span><span style="white-space:pre;"><div style="position:absolute;top:515.85pt;left:388.82pt;z-index:235;letter-spacing:.01em;">tist at the General Electric Company (GEC), ﬁrst at</div></span><span style="white-space:pre;"><div style="position:absolute;top:524.82pt;left:388.82pt;z-index:236;letter-spacing:-.012em;">Marconi  Command  and  Control  Systems  (MCCS),</div></span><span style="white-space:pre;"><div style="position:absolute;top:533.79pt;left:388.82pt;z-index:237;letter-spacing:.014em;">Frimley, and then, from 1989, at the GEC Marconi</div></span><span style="white-space:pre;"><div style="position:absolute;top:542.75pt;left:388.82pt;z-index:238;letter-spacing:-.005em;">Hirst Research Centre, London. During 1995, he was</div></span><span style="white-space:pre;"><div style="position:absolute;top:551.71pt;left:388.82pt;z-index:239;letter-spacing:.011em;">a Lecturer in the Department of Computer Science,</div></span><span style="white-space:pre;"><div style="position:absolute;top:560.69pt;left:305.14pt;z-index:240;letter-spacing:.01em;">University of Reading, Reading, U.K. Since 2004, he has been a Professor in</div></span><span style="white-space:pre;"><div style="position:absolute;top:569.65pt;left:305.14pt;z-index:241;letter-spacing:.007em;">the School of Computer Science and Information Systems, Birkbeck College,</div></span><span style="white-space:pre;"><div style="position:absolute;top:578.61pt;left:305.14pt;z-index:242;letter-spacing:-.005em;">University of London. His current research interests include camera calibration,</div></span><span style="white-space:pre;"><div style="position:absolute;top:587.58pt;left:305.14pt;z-index:243;letter-spacing:.011em;">visual surveillance, tracking, ﬁltering, applications of projective geometry to</div></span><span style="white-space:pre;"><div style="position:absolute;top:596.55pt;left:305.14pt;z-index:244;letter-spacing:-.001em;">computer vision and applications of probability, statistics, and information the-</div></span><span style="white-space:pre;"><div style="position:absolute;top:605.52pt;left:305.14pt;z-index:245;letter-spacing:.004em;">ory to computer vision. He is the author or coauthor of more than 90 scientiﬁc</div></span><span style="white-space:pre;"><div style="position:absolute;top:614.48pt;left:305.14pt;z-index:246;">publications and one book.</div></span><span style="white-space:pre;"><div style="position:absolute;top:623.44pt;left:315.31pt;z-index:247;letter-spacing:.005em;">Prof. Maybank is a Fellow of the Institute of Mathematics and Its Applica-</div></span><span style="white-space:pre;"><div style="position:absolute;top:632.42pt;left:305.14pt;z-index:248;letter-spacing:-.001em;">tions and the Royal Statistical Society.</div></span><span class="fmt-22" style="white-space:pre;"><div style="position:absolute;top:656.32pt;left:388.82pt;z-index:251;letter-spacing:-.003em;">Shuicheng Yan  <span class="fmt-7">(M’06) received the B.S. and Ph.D.</span></div></span><img style="position:absolute;left:305.14pt;top:657.98pt;width:71.7pt;height:89.625pt;z-index:249;" src="./target/0639d671c236a9d83288f256d7f9c8fc.png" alt="Image_75_0" /><span style="white-space:pre;"><div style="position:absolute;top:665.29pt;left:388.82pt;z-index:252;letter-spacing:-.015em;">degrees  from  Peking  University,  Beijing,  China,  in</div></span><span style="white-space:pre;"><div style="position:absolute;top:674.26pt;left:388.82pt;z-index:253;letter-spacing:-.004em;">1999 and 2004, respectively.</div></span><span style="white-space:pre;"><div style="position:absolute;top:683.22pt;left:399.95pt;z-index:254;letter-spacing:-.013em;">He  is  currently  an  Assistant  Professor  with  the</div></span><span style="white-space:pre;"><div style="position:absolute;top:692.19pt;left:388.82pt;z-index:255;letter-spacing:-.004em;">Department of Electrical and Computer Engineering,</div></span><span style="white-space:pre;"><div style="position:absolute;top:701.16pt;left:388.82pt;z-index:256;letter-spacing:-.002em;">National University of Singapore, Singapore. His re-</div></span><span style="white-space:pre;"><div style="position:absolute;top:710.12pt;left:388.82pt;z-index:257;letter-spacing:-.003em;">search  interests  include  computer  vision,  machine</div></span><span style="white-space:pre;"><div style="position:absolute;top:719.09pt;left:388.82pt;z-index:258;">learning, and data mining.</div></span></div>
      </div>
      <div style="width:100%">
        <hr />
        <table style="border:0;width:100%;">
          <tbody>
            <tr>
              <td bgcolor="eeeeee" align="right">
                <font face="arial,sans-serif">
                  <b>Page 11</b>
                </font>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <div style="position:relative;width:594pt;height:792pt;">
        <div style="position:absolute;left:0pt;top:0pt;width:100%;height:100%;clip:rect(0pt,594pt,792pt,0pt);" class="fmt-7"><span style="white-space:pre;"><div style="position:absolute;top:166.16pt;left:300.92pt;z-index:42;letter-spacing:.001em;">pattern recognition, statistical learning, and multimedia content analysis.</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:37.908pt;z-index:3;letter-spacing:-.001em;">LI <span class="fmt-10">et al.</span>: GAIT COMPONENTS AND THEIR APPLICATION TO GENDER RECOGNITION</div></span><span class="fmt-22" style="white-space:pre;"><div style="position:absolute;top:67.5326pt;left:121.59pt;z-index:7;letter-spacing:-.006em;">Dacheng  Tao  <span class="fmt-7">(M’07)  received  the  B.Eng.  degree</span></div></span><img style="position:absolute;left:37.908pt;top:69.1949pt;width:71.7pt;height:89.625pt;z-index:5;" src="./target/63b25ac74c732893b29487a14cad1e9b.png" alt="Image_112_0" /><span style="white-space:pre;"><div style="position:absolute;top:76.4965pt;left:121.59pt;z-index:8;letter-spacing:-.016em;">from  the  University  of  Science  and  Technology  of</div></span><span style="white-space:pre;"><div style="position:absolute;top:85.4693pt;left:121.59pt;z-index:9;letter-spacing:.011em;">China,  Hefei,  the  M.Phil.  degree  from  the  Chi-</div></span><span style="white-space:pre;"><div style="position:absolute;top:94.4332pt;left:121.59pt;z-index:10;letter-spacing:-.003em;">nese  University  of  Hong  Kong,  Hong  Kong,  and</div></span><span style="white-space:pre;"><div style="position:absolute;top:103.4pt;left:121.59pt;z-index:11;letter-spacing:-.004em;">the  Ph.D.  degree  from  the  University  of  London,</div></span><span style="white-space:pre;"><div style="position:absolute;top:112.36pt;left:121.59pt;z-index:12;">London, U.K.</div></span><span style="white-space:pre;"><div style="position:absolute;top:121.33pt;left:131.34pt;z-index:13;letter-spacing:-.004em;">He is currently an Assistant Professor at the Hong</div></span><span style="white-space:pre;"><div style="position:absolute;top:130.3pt;left:121.59pt;z-index:14;letter-spacing:-.007em;">Kong Polytechnic University, Kowloon. His research</div></span><span style="white-space:pre;"><div style="position:absolute;top:139.26pt;left:121.59pt;z-index:15;letter-spacing:.007em;">interests include artiﬁcial intelligence, computer vi-</div></span><span style="white-space:pre;"><div style="position:absolute;top:148.23pt;left:121.59pt;z-index:16;letter-spacing:-.007em;">sion, data mining, information theory, machine learn-</div></span><span style="white-space:pre;"><div style="position:absolute;top:157.2pt;left:121.59pt;z-index:17;letter-spacing:-.006em;">ing, and visual surveillance. He is the author or coau-</div></span><span style="white-space:pre;"><div style="position:absolute;top:166.16pt;left:37.9079pt;z-index:18;letter-spacing:-.01em;">thor of various papers published in several international journals. He is the editor</div></span><span style="white-space:pre;"><div style="position:absolute;top:175.13pt;left:37.9079pt;z-index:19;letter-spacing:-.003em;">of two books and a guest editor of six journals. He is an Associate Editor of the</div></span><span class="fmt-21" style="white-space:pre;"><div style="position:absolute;top:184.09pt;left:37.9079pt;z-index:21;letter-spacing:-.002em;">Neurocomputing <span class="fmt-7">(Elsevier) journal.</span></div></span><span style="white-space:pre;"><div style="position:absolute;top:193.06pt;left:48.3209pt;z-index:22;letter-spacing:.007em;">Dr. Tao was the Co-Chaire of the Special Session on Information Security</div></span><span style="white-space:pre;"><div style="position:absolute;top:202.03pt;left:37.9079pt;z-index:23;letter-spacing:.016em;">at the IEEE International Conference on Machine Learning and Cybernetics</div></span><span style="white-space:pre;"><div style="position:absolute;top:210.99pt;left:37.9079pt;z-index:24;letter-spacing:.006em;">(ICMLC) and the Workshop on Knowledge Discovery and Data Mining from</div></span><span style="white-space:pre;"><div style="position:absolute;top:219.96pt;left:37.9079pt;z-index:25;letter-spacing:.004em;">Multimedia Data and Multimedia Applications at the IEEE International Con-</div></span><span style="white-space:pre;"><div style="position:absolute;top:228.93pt;left:37.9079pt;z-index:26;letter-spacing:-.006em;">ference on Data Mining. He is the recipient of several Meritorious Awards from</div></span><span style="white-space:pre;"><div style="position:absolute;top:237.89pt;left:37.9079pt;z-index:27;letter-spacing:.006em;">the International Interdisciplinary Contest in Modeling organized by the Con-</div></span><span style="white-space:pre;"><div style="position:absolute;top:246.86pt;left:37.9079pt;z-index:28;">sortium for Mathematics and Its Applications.</div></span><span class="fmt-0" style="white-space:pre;"><div style="position:absolute;top:34.3462pt;left:541.54pt;z-index:4;letter-spacing:-.001em;">155</div></span><span class="fmt-22" style="white-space:pre;"><div style="position:absolute;top:67.5326pt;left:384.61pt;z-index:31;letter-spacing:.004em;">Dong Xu <span class="fmt-7">(M’07) received the B.Eng. and Ph.D. de-</span></div></span><img style="position:absolute;left:300.92pt;top:69.1949pt;width:71.7pt;height:89.625pt;z-index:29;" src="./target/af7e9fd5d048c4307a81a31a6d0b9832.png" alt="Image_18_0" /><span style="white-space:pre;"><div style="position:absolute;top:76.4965pt;left:384.61pt;z-index:32;letter-spacing:-.008em;">grees from the University of Science and Technology</div></span><span style="white-space:pre;"><div style="position:absolute;top:85.4693pt;left:384.61pt;z-index:33;letter-spacing:-.002em;">of China, Hefei, in 2001 and 2005, respectively.</div></span><span style="white-space:pre;"><div style="position:absolute;top:94.4332pt;left:395.74pt;z-index:34;letter-spacing:-.013em;">He  is  currently  an  Assistant  Professor  with  the</div></span><span style="white-space:pre;"><div style="position:absolute;top:103.4pt;left:384.61pt;z-index:35;letter-spacing:.002em;">School of Computer Engineering, Nanyang Techno-</div></span><span style="white-space:pre;"><div style="position:absolute;top:112.36pt;left:384.61pt;z-index:36;letter-spacing:-.005em;">logical University, Singapore. During his Ph.D. stud-</div></span><span style="white-space:pre;"><div style="position:absolute;top:121.33pt;left:384.61pt;z-index:37;letter-spacing:.015em;">ies, he was with Microsoft Research Asia, Beijing,</div></span><span style="white-space:pre;"><div style="position:absolute;top:130.3pt;left:384.61pt;z-index:38;letter-spacing:-.016em;">China  and  The  Chinese  University  of  Hong  Kong,</div></span><span style="white-space:pre;"><div style="position:absolute;top:139.26pt;left:384.61pt;z-index:39;letter-spacing:.002em;">Shatin. He has also spent one year at Columbia Uni-</div></span><span style="white-space:pre;"><div style="position:absolute;top:148.23pt;left:384.61pt;z-index:40;letter-spacing:-.007em;">versity, New York, as a Postdoctoral Research Scien-</div></span><span style="white-space:pre;"><div style="position:absolute;top:157.2pt;left:384.61pt;z-index:41;letter-spacing:.011em;">tist. His research interests include computer vision,</div></span></div>
      </div>
    </div>
  </body>
</html>